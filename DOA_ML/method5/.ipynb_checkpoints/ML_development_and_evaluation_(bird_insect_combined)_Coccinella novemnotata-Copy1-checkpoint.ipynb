{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b8249e7",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#일반-log-정규화만\" data-toc-modified-id=\"일반-log-정규화만-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>일반 log 정규화만</a></span><ul class=\"toc-item\"><li><span><a href=\"#변수-추출\" data-toc-modified-id=\"변수-추출-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>변수 추출</a></span><ul class=\"toc-item\"><li><span><a href=\"#통계적-검증\" data-toc-modified-id=\"통계적-검증-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>통계적 검증</a></span><ul class=\"toc-item\"><li><span><a href=\"#독립선형회귀분석(scipy)을-거친-경우\" data-toc-modified-id=\"독립선형회귀분석(scipy)을-거친-경우-1.1.1.1\"><span class=\"toc-item-num\">1.1.1.1&nbsp;&nbsp;</span>독립선형회귀분석(scipy)을 거친 경우</a></span></li><li><span><a href=\"#처음부터-다중선형회귀분석으로-직행하는-경우\" data-toc-modified-id=\"처음부터-다중선형회귀분석으로-직행하는-경우-1.1.1.2\"><span class=\"toc-item-num\">1.1.1.2&nbsp;&nbsp;</span>처음부터 다중선형회귀분석으로 직행하는 경우</a></span></li><li><span><a href=\"#결과-종합\" data-toc-modified-id=\"결과-종합-1.1.1.3\"><span class=\"toc-item-num\">1.1.1.3&nbsp;&nbsp;</span>결과 종합</a></span></li><li><span><a href=\"#다중공산성\" data-toc-modified-id=\"다중공산성-1.1.1.4\"><span class=\"toc-item-num\">1.1.1.4&nbsp;&nbsp;</span>다중공산성</a></span></li></ul></li></ul></li><li><span><a href=\"#머신러닝\" data-toc-modified-id=\"머신러닝-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>머신러닝</a></span><ul class=\"toc-item\"><li><span><a href=\"#압축-전(변수-10개)\" data-toc-modified-id=\"압축-전(변수-10개)-1.2.1\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>압축 전(변수 10개)</a></span></li></ul></li><li><span><a href=\"#check-point!\" data-toc-modified-id=\"check-point!-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>check point!</a></span></li></ul></li><li><span><a href=\"#머신러닝\" data-toc-modified-id=\"머신러닝-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>머신러닝</a></span></li><li><span><a href=\"#SHAP\" data-toc-modified-id=\"SHAP-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>SHAP</a></span><ul class=\"toc-item\"><li><span><a href=\"#feature-importance-selector\" data-toc-modified-id=\"feature-importance-selector-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>feature importance selector</a></span></li></ul></li><li><span><a href=\"#그-밖의-통계-결과\" data-toc-modified-id=\"그-밖의-통계-결과-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>그 밖의 통계 결과</a></span><ul class=\"toc-item\"><li><span><a href=\"#결정트리\" data-toc-modified-id=\"결정트리-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>결정트리</a></span></li><li><span><a href=\"#앙상블학습\" data-toc-modified-id=\"앙상블학습-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>앙상블학습</a></span></li><li><span><a href=\"#랜덤-포레스트\" data-toc-modified-id=\"랜덤-포레스트-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>랜덤 포레스트</a></span><ul class=\"toc-item\"><li><span><a href=\"#GBM\" data-toc-modified-id=\"GBM-4.3.1\"><span class=\"toc-item-num\">4.3.1&nbsp;&nbsp;</span>GBM</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88dbcd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imbalanced-learn 패키지\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c6ed9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from urllib.request import Request # 서버 요청 객체를 생성하는 모듈\n",
    "import urllib.request\n",
    "from tqdm import tqdm_notebook # 반복문의 반복 요소에 적용시키면 반복요소가 얼마나 진행되었는지 상태바를 표시\n",
    "from tqdm import tqdm\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity=\"all\"\n",
    "from IPython.display import Image\n",
    "from tqdm import tqdm_notebook\n",
    "import matplotlib.pyplot as plt #그래프 패키지 모듈 등록\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "\n",
    "import pandas\n",
    "from haversine import haversine\n",
    "\n",
    "pd.set_option('display.max_columns', 10)\n",
    "pd.set_option('display.max_rows', 10)\n",
    "pd.set_option('display.max_seq_items', None) # 한 줄 내 표현\n",
    "\n",
    "from sklearn.datasets import load_iris # sklearn.datasets - 자체 제공 데이터\n",
    "from sklearn.tree import DecisionTreeClassifier # sklearn.알고리즘명 - ML 알고리즘을 구현한 클래스 집합\n",
    "from sklearn.model_selection import train_test_split # sklearn.model_selection - 데이터 분리, 최적의 하이퍼파라미터 평가 등\n",
    "from sklearn.metrics import accuracy_score # sklearn.metrics 평가 등\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from patsy import dmatrices\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from statsmodels.formula.api import ols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dada8cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import plot_importance\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "\n",
    "# 사이킷런 래퍼 XGBoost 클래스인 XGBClassifier 임포트\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import pygraphviz as pgv\n",
    "\n",
    "from sklearn.tree import export_graphviz\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def get_clf_eval(y_test, pred=None, pred_proba=None):\n",
    "    confusion = confusion_matrix( y_test, pred)\n",
    "    accuracy = accuracy_score(y_test , pred)\n",
    "    precision = precision_score(y_test , pred)\n",
    "    recall = recall_score(y_test , pred)\n",
    "    f1 = f1_score(y_test,pred)\n",
    "    # ROC-AUC 추가 \n",
    "    roc_auc = roc_auc_score(y_test, pred_proba)\n",
    "    print('오차 행렬')\n",
    "    print(confusion)\n",
    "    # ROC-AUC print 추가\n",
    "    print('정확도: {0:.4f}, 정밀도: {1:.4f}, 재현율: {2:.4f},\\\n",
    "    F1: {3:.4f}, AUC:{4:.4f}'.format(accuracy, precision, recall, f1, roc_auc))\n",
    "    \n",
    "\n",
    "def get_clf_eval_num(y_test, pred=None, pred_proba=None):\n",
    "    confusion = confusion_matrix( y_test, pred)\n",
    "    accuracy = accuracy_score(y_test , pred)\n",
    "    precision = precision_score(y_test , pred)\n",
    "    recall = recall_score(y_test , pred)\n",
    "    f1 = f1_score(y_test,pred)\n",
    "    # ROC-AUC 추가 \n",
    "    roc_auc = roc_auc_score(y_test, pred_proba)\n",
    "    \n",
    "    return accuracy, precision, recall, f1, roc_auc\n",
    "\n",
    "\n",
    "def get_clf_eval_num_easy(y_test, pred=None):\n",
    "    confusion = confusion_matrix( y_test, pred)\n",
    "    accuracy = accuracy_score(y_test , pred)\n",
    "    precision = precision_score(y_test , pred)\n",
    "    recall = recall_score(y_test , pred)\n",
    "    f1 = f1_score(y_test,pred)\n",
    "    \n",
    "    return accuracy, precision, recall, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16eec9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clf_eval(y_test, pred=None, pred_proba=None):\n",
    "    confusion = confusion_matrix( y_test, pred)\n",
    "    accuracy = accuracy_score(y_test , pred)\n",
    "    precision = precision_score(y_test , pred)\n",
    "    recall = recall_score(y_test , pred)\n",
    "    f1 = f1_score(y_test,pred)\n",
    "    # ROC-AUC 추가 \n",
    "    roc_auc = roc_auc_score(y_test, pred_proba)\n",
    "#     print('오차 행렬')\n",
    "#     print(confusion)\n",
    "#     # ROC-AUC print 추가\n",
    "#     print('정확도: {0:.4f}, 정밀도: {1:.4f}, 재현율: {2:.4f},\\\n",
    "#     F1: {3:.4f}, AUC:{4:.4f}'.format(accuracy, precision, recall, f1, roc_auc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04710fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "allpot = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e305e2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "InteractiveShell.ast_node_interactivity=\"all\"\n",
    "#InteractiveShell.ast_node_interactivity=\"last_expr\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12888ee6",
   "metadata": {},
   "source": [
    "# 일반 log 정규화만"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6772917a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MINMAX scaling\n",
    "c92 = pd.read_csv('/Users/hyun-yong/JPnotebook/LDYcol/10.csv for ML_new/20220220_최신 데이터 세트/220220_DOA_c9(최종,minmax).csv', index_col = 0)\n",
    "\n",
    "# log scaling\n",
    "# c92 = pd.read_csv('/Users/hyun-yong/JPnotebook/LDYcol/10.csv for ML_new/20220220_최신 데이터 세트/220220_DOA_c9(최종,log).csv', index_col = 0)\n",
    "\n",
    "######(option) 반경 18 km 이내에 C9있는 absence 제외\n",
    "c92 = c92.drop(c92[(c92['Coccinella novemnotata_18']!=0)&(c92['Species']!='Coccinella novemnotata')].index)\n",
    "\n",
    "\n",
    "#### 히포다이마 제와 및 C9 + axy만 absence인 버전\n",
    "c92 = c92.drop(c92[c92['Species'] == 'Hippodamia convergens'].index).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13732b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "c92.columns\n",
    "\n",
    "# c92 = c92[c92.Year > 2008].reset_index(drop=True)\n",
    "\n",
    "c92 = c92.sort_values(by='Species').reset_index(drop=True)\n",
    "\n",
    "c92_n = c92[['Species','Harmonia axyridis_18',\n",
    "       'Coccinella septempunctata_18', 'Hippodamia convergens_18',\n",
    "       'Coelophora maculata_18', 'Cycloneda sanguinea_18',\n",
    "       'Propylea quatuordecimpunctata_18', 'Cycloneda munda_18',\n",
    "       'Psyllobora vigintimaculata_18', 'Chilocorus stigma_18',\n",
    "       'Olla v-nigrum_18', 'Hippodamia variegata_18', 'Adalia bipunctata_18',\n",
    "       'Coccinella californica_18', 'Cycloneda polita_18',\n",
    "       'Cryptolaemus montrouzieri_18', 'Coccinella trifasciata_18',\n",
    "       'Epilachna borealis_18', 'Anatis mali_18', 'Chilocorus cacti_18',\n",
    "       'Brachiacantha ursina_18', 'Hippodamia parenthesis_18',\n",
    "       'Anatis labiculata_18', 'Mulsantina picta_18',\n",
    "       'Coccinella transversoguttata_18', 'Psyllobora renifer_18',\n",
    "       'Naemia seriata_18', 'Hippodamia tredecimpunctata_18',\n",
    "       'Epilachna varivestis_18', # 'Coccinella novemnotata_18',\n",
    "       'Hippodamia glacialis_18', 'Anatis lecontei_18', 'Axion plagiatum_18',\n",
    "       'Calvia quatuordecimguttata_18', 'Coleomegilla maculata_18',\n",
    "       'Stethorus punctillum_18', 'Anatis rathvoni_18',\n",
    "       'Psyllobora borealis_18', 'Curinus coeruleus_18', 'Myzia pullata_18',\n",
    "       'Halmus chalybeus_18', 'Neoharmonia venusta_18',\n",
    "       'Exochomus childreni_18', 'Hippodamia caseyi_18',\n",
    "       'Coelophora inaequalis_18', 'Coccinella monticola_18',\n",
    "       'Mulsantina hudsonica_18', 'Hyperaspis binotata_18',\n",
    "       'Diomus terminatus_18', 'Anisosticta bitriangularis_18',\n",
    "       'Exochomus quadripustulatus_18', 'Hippodamia sinuata_18',\n",
    "       'Brumoides septentrionis_18', 'Hippodamia quinquesignata_18',\n",
    "       'Hyperaspis bigeminata_18', 'Paranaemia vittigera_18',\n",
    "       'Chilocorus bipustulatus_18', 'Rhyzobius lophanthae_18',\n",
    "       'Novius cardinalis_18', 'Brachiacantha decempustulata_18',\n",
    "       'Scymnus loewii_18', 'Scymnus nebulosus_18', 'Didion punctatum_18',\n",
    "       'Stethorus punctum_18', 'Myzia interrupta_18',\n",
    "       'Hyperaspis trifurcata_18', 'Axion tripustulatum_18',\n",
    "       'Epilachna tredecimnotata_18',\n",
    "       'Subcoccinella vigintiquatuorpunctata_18', 'Hippodamia apicalis_18',\n",
    "       'Hyperaspis undulata_18', 'Exochomus aethiops_18',\n",
    "       'Hyperaspis lateralis_18', 'Coccinella hieroglyphica_18',\n",
    "       'Hyperaspis quadrioculata_18', 'Exochomus marginipennis_18',\n",
    "       'Microweisea misella_18', 'Hippodamia oregonensis_18',\n",
    "       'Brachiacantha dentipes_18', 'Myzia subvittata_18',\n",
    "       'Psyllobora parvinotata_18', 'Nephus flavifrons_18',\n",
    "       'Brachiacantha quadripunctata_18', 'Hyperaspis proba_18',\n",
    "       'Egius platycephalus_18', 'Hyperaspis connectens_18',\n",
    "       'Nephus intrusus_18', 'Coccinella undecimpunctata_18',\n",
    "       'Exochomus fasciatus_18', 'Scymnus louisianae_18',\n",
    "       'Delphastus pusillus_18', 'Eremophila alpestris_18',\n",
    "       'Lanius ludovicianus_18', 'Plectrophenax nivalis_18',\n",
    "       'Phainopepla nitens_18', 'Lanius borealis_18',\n",
    "       'Calcarius lapponicus_18', 'Cinclus mexicanus_18',\n",
    "       'Lonchura punctulata_18', 'Zosterops japonicus_18',\n",
    "       'Centronyx henslowii_18', 'Paroaria coronata_18',\n",
    "       'Calcarius ornatus_18', 'Passer montanus_18', 'Pycnonotus cafer_18',\n",
    "       'Sicalis flaveola_18', 'Estrilda astrild_18', 'Pycnonotus jocosus_18',\n",
    "       'Copsychus malabaricus_18', 'Rhynchophanes mccownii_18',\n",
    "       'Paroaria capitata_18', 'Leiothrix lutea_18',\n",
    "       'Peucedramus taeniatus_18', 'Lonchura atricapilla_18',\n",
    "       'Centronyx bairdii_18', 'Chasiempis sandwichensis_18',\n",
    "       'Euodice cantans_18', 'Vidua macroura_18', 'Horornis diphone_18',\n",
    "       'Garrulax canorus_18', 'Calcarius pictus_18', 'Zosterops simplex_18',\n",
    "       'Alauda arvensis_18', 'Oenanthe oenanthe_18',\n",
    "       'Euplectes franciscanus_18', 'Phylloscopus borealis_18',\n",
    "       'Amandava amandava_18', 'Chasiempis sclateri_18',\n",
    "       'Sporophila torqueola_18', 'Luscinia svecica_18', 'Estrilda melpoda_18',\n",
    "       'Chasiempis ibidis_18', 'Pachyramphus aglaiae_18',\n",
    "       'Plectrophenax hyperboreus_18', 'Prunella montanella_18',\n",
    "       'Spindalis zena_18', 'Tarsiger cyanurus_18', 'Coereba flaveola_18',\n",
    "       'Tiaris olivaceus_18', 'Phylloscopus fuscatus_18', 'Lanius collurio_18',\n",
    "       'Muscicapa griseisticta_18', 'Phylloscopus inornatus_18',\n",
    "       'Lanius cristatus_18', 'Acrocephalus familiaris_18',\n",
    "       'Phylloscopus trochilus_18', 'Muscicapa sibirica_18',\n",
    "       'Phylloscopus collybita_18', 'Ficedula albicilla_18',\n",
    "       'Estrilda troglodytes_18', 'Lonchura malacca_18', 'Saxicola maurus_18',\n",
    "       'Larvivora sibilans_18', 'Erithacus rubecula_18',\n",
    "       'Phylloscopus examinandus_18', 'Locustella lanceolata_18',\n",
    "       'Acrocephalus dumetorum_18', 'Cyanerpes cyaneus_18',\n",
    "       'Acrocephalus schoenobaenus_18', 'Luscinia cyane_18',\n",
    "       'Oenanthe pleschanka_18', 'Uraeginthus bengalus_18',\n",
    "       'Phoenicurus phoenicurus_18', 'Pachyramphus major_18',\n",
    "       'Taeniopygia guttata_18', 'Locustella fluviatilis_18']]\n",
    "\n",
    "\n",
    "c92_n['Species']\n",
    "        \n",
    "c92_n['Species_1'] = c92_n['Species']\n",
    "c92_n['Species_18'] = c92_n['Species']\n",
    "\n",
    "\n",
    "c92_05 = c92_n.filter(regex='0.5')\n",
    "c92_1 = c92_n.filter(regex='_1') # 이거 18과 구분 필요\n",
    "c92_2 = c92_n.filter(regex='_2')\n",
    "c92_4 = c92_n.filter(regex='_4')\n",
    "c92_8 = c92_n.filter(regex='_8')\n",
    "c92_18 = c92_n.filter(regex='_18')\n",
    "\n",
    "c92_1 = c92_n[list(set(c92_1.columns) - set(c92_18.columns))]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5ff92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "c92_1['target'] = 0\n",
    "for i in c92_1.index:\n",
    "    if c92_1['Species_1'][i] == 'Coccinella novemnotata':\n",
    "        c92_1['target'][i] = 1\n",
    "\n",
    "c92_1['target']\n",
    "\n",
    "\n",
    "c92_18['target'] = 0\n",
    "for i in c92_18.index:\n",
    "    if c92_18['Species_18'][i] == 'Coccinella novemnotata':\n",
    "        c92_18['target'][i] = 1\n",
    "\n",
    "c92_18['target']\n",
    "\n",
    "c92zet = pd.merge(c92[['Source', 'Year', 'Species', 'Latitude', 'Longitude',\n",
    "       'public_positional_accuracy', 'State', 'gps']], c92_18, left_index=True, right_index=True, how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d2a8d9",
   "metadata": {},
   "source": [
    "## 변수 추출"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2647605",
   "metadata": {},
   "source": [
    "### 통계적 검증"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52131bf",
   "metadata": {},
   "source": [
    "#### 독립선형회귀분석(scipy)을 거친 경우"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52653847",
   "metadata": {},
   "source": [
    "##### 회귀분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe70e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "c92_18.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa45338",
   "metadata": {},
   "outputs": [],
   "source": [
    "c92_18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e919039",
   "metadata": {},
   "outputs": [],
   "source": [
    "c92 = c92_18.copy()\n",
    "\n",
    "# 관측건수 50으로 자른후의 회귀분석결과\n",
    "\n",
    "\n",
    "from scipy.stats import *\n",
    "\n",
    "jazz = ['Harmonia axyridis_18', 'Coccinella septempunctata_18',\n",
    "       'Hippodamia convergens_18', 'Coelophora maculata_18',\n",
    "       'Cycloneda sanguinea_18', 'Propylea quatuordecimpunctata_18',\n",
    "       'Cycloneda munda_18', 'Psyllobora vigintimaculata_18',\n",
    "       'Chilocorus stigma_18', 'Olla v-nigrum_18', 'Hippodamia variegata_18',\n",
    "       'Adalia bipunctata_18', 'Coccinella californica_18',\n",
    "       'Cycloneda polita_18', 'Cryptolaemus montrouzieri_18',\n",
    "       'Coccinella trifasciata_18', 'Epilachna borealis_18', 'Anatis mali_18',\n",
    "       'Chilocorus cacti_18', 'Brachiacantha ursina_18',\n",
    "       'Hippodamia parenthesis_18', 'Anatis labiculata_18',\n",
    "       'Mulsantina picta_18', 'Coccinella transversoguttata_18',\n",
    "       'Psyllobora renifer_18', 'Naemia seriata_18',\n",
    "       'Hippodamia tredecimpunctata_18', 'Epilachna varivestis_18',\n",
    "       'Hippodamia glacialis_18', 'Anatis lecontei_18', 'Axion plagiatum_18',\n",
    "       'Calvia quatuordecimguttata_18', 'Coleomegilla maculata_18',\n",
    "       'Stethorus punctillum_18', 'Anatis rathvoni_18',\n",
    "       'Psyllobora borealis_18', 'Curinus coeruleus_18', 'Myzia pullata_18',\n",
    "       'Halmus chalybeus_18', 'Neoharmonia venusta_18',\n",
    "       'Exochomus childreni_18', 'Hippodamia caseyi_18',\n",
    "       'Coelophora inaequalis_18', 'Coccinella monticola_18',\n",
    "       'Mulsantina hudsonica_18', 'Hyperaspis binotata_18',\n",
    "       'Diomus terminatus_18', 'Anisosticta bitriangularis_18',\n",
    "       'Exochomus quadripustulatus_18', 'Hippodamia sinuata_18',\n",
    "       'Brumoides septentrionis_18', 'Hippodamia quinquesignata_18',\n",
    "       'Hyperaspis bigeminata_18', 'Paranaemia vittigera_18',\n",
    "       'Chilocorus bipustulatus_18', 'Rhyzobius lophanthae_18',\n",
    "       'Novius cardinalis_18', 'Brachiacantha decempustulata_18',\n",
    "       'Scymnus loewii_18', 'Scymnus nebulosus_18', 'Didion punctatum_18',\n",
    "       'Stethorus punctum_18', 'Myzia interrupta_18',\n",
    "       'Hyperaspis trifurcata_18', 'Axion tripustulatum_18',\n",
    "       'Epilachna tredecimnotata_18',\n",
    "       'Subcoccinella vigintiquatuorpunctata_18', 'Hippodamia apicalis_18',\n",
    "       'Hyperaspis undulata_18', 'Exochomus aethiops_18',\n",
    "       'Hyperaspis lateralis_18', 'Coccinella hieroglyphica_18',\n",
    "       'Hyperaspis quadrioculata_18', 'Exochomus marginipennis_18',\n",
    "       'Microweisea misella_18', 'Hippodamia oregonensis_18',\n",
    "       'Brachiacantha dentipes_18', 'Myzia subvittata_18',\n",
    "       'Psyllobora parvinotata_18', 'Nephus flavifrons_18',\n",
    "       'Brachiacantha quadripunctata_18', 'Hyperaspis proba_18',\n",
    "       'Egius platycephalus_18', 'Hyperaspis connectens_18',\n",
    "       'Nephus intrusus_18', 'Coccinella undecimpunctata_18',\n",
    "       'Exochomus fasciatus_18', 'Scymnus louisianae_18',\n",
    "       'Delphastus pusillus_18', 'Eremophila alpestris_18',\n",
    "       'Lanius ludovicianus_18', 'Plectrophenax nivalis_18',\n",
    "       'Phainopepla nitens_18', 'Lanius borealis_18',\n",
    "       'Calcarius lapponicus_18', 'Cinclus mexicanus_18',\n",
    "       'Lonchura punctulata_18', 'Zosterops japonicus_18',\n",
    "       'Centronyx henslowii_18', 'Paroaria coronata_18',\n",
    "       'Calcarius ornatus_18', 'Passer montanus_18', 'Pycnonotus cafer_18',\n",
    "       'Sicalis flaveola_18', 'Estrilda astrild_18', 'Pycnonotus jocosus_18',\n",
    "       'Copsychus malabaricus_18', 'Rhynchophanes mccownii_18',\n",
    "       'Paroaria capitata_18', 'Leiothrix lutea_18',\n",
    "       'Peucedramus taeniatus_18', 'Lonchura atricapilla_18',\n",
    "       'Centronyx bairdii_18', 'Chasiempis sandwichensis_18',\n",
    "       'Euodice cantans_18', 'Vidua macroura_18', 'Horornis diphone_18',\n",
    "       'Garrulax canorus_18', 'Calcarius pictus_18', 'Zosterops simplex_18',\n",
    "       'Alauda arvensis_18', 'Oenanthe oenanthe_18',\n",
    "       'Euplectes franciscanus_18', 'Phylloscopus borealis_18',\n",
    "       'Amandava amandava_18', 'Chasiempis sclateri_18',\n",
    "       'Sporophila torqueola_18', 'Luscinia svecica_18', 'Estrilda melpoda_18',\n",
    "       'Chasiempis ibidis_18', 'Pachyramphus aglaiae_18',\n",
    "       'Plectrophenax hyperboreus_18', 'Prunella montanella_18',\n",
    "       'Spindalis zena_18', 'Tarsiger cyanurus_18', 'Coereba flaveola_18',\n",
    "       'Tiaris olivaceus_18', 'Phylloscopus fuscatus_18', 'Lanius collurio_18',\n",
    "       'Muscicapa griseisticta_18', 'Phylloscopus inornatus_18',\n",
    "       'Lanius cristatus_18', 'Acrocephalus familiaris_18',\n",
    "       'Phylloscopus trochilus_18', 'Muscicapa sibirica_18',\n",
    "       'Phylloscopus collybita_18', 'Ficedula albicilla_18',\n",
    "       'Estrilda troglodytes_18', 'Lonchura malacca_18', 'Saxicola maurus_18',\n",
    "       'Larvivora sibilans_18', 'Erithacus rubecula_18',\n",
    "       'Phylloscopus examinandus_18', 'Locustella lanceolata_18',\n",
    "       'Acrocephalus dumetorum_18', 'Cyanerpes cyaneus_18',\n",
    "       'Acrocephalus schoenobaenus_18', 'Luscinia cyane_18',\n",
    "       'Oenanthe pleschanka_18', 'Uraeginthus bengalus_18',\n",
    "       'Phoenicurus phoenicurus_18', 'Pachyramphus major_18',\n",
    "       'Taeniopygia guttata_18', 'Locustella fluviatilis_18'] #, 'target']\n",
    "\n",
    "pot = []\n",
    "for i in jazz:\n",
    "    model = stats.linregress(c92[i], c92['target'])\n",
    "    print(i)\n",
    "    print(\"\\n model: \", model)\n",
    "    print(\"기울기: \", model.slope)\n",
    "    print(\"절편: \", model.intercept) \n",
    "    print(\"상관계수: \", model.rvalue) \n",
    "    print(\"p값: \", model.pvalue)\n",
    "    print(\"표준오차: \", model.stderr)\n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "    if model.pvalue < 0.05:\n",
    "        pot.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5482d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb8837e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) 독립선형회귀분석을 거친 경우\n",
    "lemon=[]\n",
    "for i in pot:\n",
    "    lemon.append(i.replace(' ','_').replace('-',''))\n",
    "\n",
    "    \n",
    "lemon2 = ' + '.join(lemon)\n",
    "lemon\n",
    "lemon2\n",
    "\n",
    "for i in c92.columns:\n",
    "    for j in range(0, len(pot)):\n",
    "        if i == pot[j]:\n",
    "            c92.rename(columns={i: lemon[j]}, inplace = True)\n",
    "\n",
    "c92.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f776ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = ols('target ~ ' + lemon2, data=c92).fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b08bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "green_moss = pd.DataFrame(res.pvalues)\n",
    "green_moss = green_moss[green_moss[0] < 0.05]\n",
    "green_moss.drop(['Intercept'], inplace=True)\n",
    "\n",
    "add_p1 = list(green_moss.index)\n",
    "#add_p1.remove('target')\n",
    "add_p1\n",
    "\n",
    "lemon2 = ' + '.join(add_p1)\n",
    "lemon2        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a7327f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한 번 더\n",
    "res = ols('target ~ ' + lemon2, data=c92).fit()\n",
    "res.summary()\n",
    "\n",
    "green_moss = pd.DataFrame(res.pvalues)\n",
    "#green_moss = green_moss[green_moss[0] < 0.05]\n",
    "green_moss.drop(['Intercept'], inplace=True)\n",
    "\n",
    "add_p1 = list(green_moss.index)\n",
    "add_p1\n",
    "\n",
    "lemon2 = ' + '.join(add_p1)\n",
    "lemon2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96e6a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한 번 더\n",
    "res = ols('target ~ ' + lemon2, data=c92).fit()\n",
    "res.summary()\n",
    "\n",
    "green_moss = pd.DataFrame(res.pvalues)\n",
    "green_moss = green_moss[green_moss[0] < 0.05]\n",
    "green_moss.drop(['Intercept'], inplace=True)\n",
    "\n",
    "add_p1 = list(green_moss.index)\n",
    "add_p1\n",
    "\n",
    "lemon2 = ' + '.join(add_p1)\n",
    "lemon2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcc2f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한 번 더\n",
    "res = ols('target ~ ' + lemon2, data=c92).fit()\n",
    "res.summary()\n",
    "\n",
    "green_moss = pd.DataFrame(res.pvalues)\n",
    "green_moss = green_moss[green_moss[0] < 0.05]\n",
    "green_moss.drop(['Intercept'], inplace=True)\n",
    "\n",
    "add_p1 = list(green_moss.index)\n",
    "add_p1\n",
    "\n",
    "lemon2 = ' + '.join(add_p1)\n",
    "lemon2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb326897",
   "metadata": {},
   "source": [
    "##### 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d3347d",
   "metadata": {},
   "outputs": [],
   "source": [
    "green_moss = pd.DataFrame(res.pvalues)\n",
    "green_moss = green_moss[green_moss[0] < 0.05]\n",
    "green_moss.drop(['Intercept'], inplace=True)\n",
    "\n",
    "add_p1 = list(green_moss.index)\n",
    "add_p1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32980d1d",
   "metadata": {},
   "source": [
    "#### 처음부터 다중선형회귀분석으로 직행하는 경우"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45df13d",
   "metadata": {},
   "source": [
    "##### 수가 많은 종 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bcb2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "c92_18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace9e585",
   "metadata": {},
   "outputs": [],
   "source": [
    "c92_copy2 = c92_18.copy()\n",
    "\n",
    "c92_copy2.columns\n",
    "\n",
    "# 2) 처음부터 다중선형회귀분석으로 직행하는 경우\n",
    "lemon=[]\n",
    "for i in c92_copy2.columns[:len(c92_copy2.columns)-1]:\n",
    "    lemon.append(i.replace(' ','_').replace('-',''))\n",
    "len(lemon)  \n",
    "\n",
    "rime = ' + '.join(lemon)\n",
    "rime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446f5834",
   "metadata": {},
   "outputs": [],
   "source": [
    "rime = 'Harmonia_axyridis_18 + Coccinella_septempunctata_18 + Hippodamia_convergens_18 + Coelophora_maculata_18 + Cycloneda_sanguinea_18 + Propylea_quatuordecimpunctata_18 + Cycloneda_munda_18 + Psyllobora_vigintimaculata_18 + Chilocorus_stigma_18 + Olla_vnigrum_18 + Hippodamia_variegata_18 + Adalia_bipunctata_18 + Coccinella_californica_18 + Cycloneda_polita_18 + Cryptolaemus_montrouzieri_18 + Coccinella_trifasciata_18 + Epilachna_borealis_18 + Anatis_mali_18 + Chilocorus_cacti_18 + Brachiacantha_ursina_18 + Hippodamia_parenthesis_18 + Anatis_labiculata_18 + Mulsantina_picta_18 + Coccinella_transversoguttata_18 + Psyllobora_renifer_18 + Naemia_seriata_18 + Hippodamia_tredecimpunctata_18 + Epilachna_varivestis_18 + Hippodamia_glacialis_18 + Anatis_lecontei_18 + Axion_plagiatum_18 + Calvia_quatuordecimguttata_18 + Coleomegilla_maculata_18 + Stethorus_punctillum_18 + Anatis_rathvoni_18 + Psyllobora_borealis_18 + Curinus_coeruleus_18 + Myzia_pullata_18 + Halmus_chalybeus_18 + Neoharmonia_venusta_18 + Exochomus_childreni_18 + Hippodamia_caseyi_18 + Coelophora_inaequalis_18 + Coccinella_monticola_18 + Mulsantina_hudsonica_18 + Hyperaspis_binotata_18 + Diomus_terminatus_18 + Anisosticta_bitriangularis_18 + Exochomus_quadripustulatus_18 + Hippodamia_sinuata_18 + Brumoides_septentrionis_18 + Hippodamia_quinquesignata_18 + Hyperaspis_bigeminata_18 + Paranaemia_vittigera_18 + Chilocorus_bipustulatus_18 + Rhyzobius_lophanthae_18 + Novius_cardinalis_18 + Brachiacantha_decempustulata_18 + Scymnus_loewii_18 + Scymnus_nebulosus_18 + Didion_punctatum_18 + Stethorus_punctum_18 + Myzia_interrupta_18 + Hyperaspis_trifurcata_18 + Axion_tripustulatum_18 + Epilachna_tredecimnotata_18 + Subcoccinella_vigintiquatuorpunctata_18 + Hippodamia_apicalis_18 + Hyperaspis_undulata_18 + Exochomus_aethiops_18 + Hyperaspis_lateralis_18 + Coccinella_hieroglyphica_18 + Hyperaspis_quadrioculata_18 + Exochomus_marginipennis_18 + Microweisea_misella_18 + Hippodamia_oregonensis_18 + Brachiacantha_dentipes_18 + Myzia_subvittata_18 + Psyllobora_parvinotata_18 + Nephus_flavifrons_18 + Brachiacantha_quadripunctata_18 + Hyperaspis_proba_18 + Egius_platycephalus_18 + Hyperaspis_connectens_18 + Nephus_intrusus_18 + Coccinella_undecimpunctata_18 + Exochomus_fasciatus_18 + Scymnus_louisianae_18 + Delphastus_pusillus_18 + Eremophila_alpestris_18 + Lanius_ludovicianus_18 + Plectrophenax_nivalis_18 + Phainopepla_nitens_18 + Lanius_borealis_18 + Calcarius_lapponicus_18 + Cinclus_mexicanus_18 + Lonchura_punctulata_18 + Zosterops_japonicus_18 + Centronyx_henslowii_18 + Paroaria_coronata_18 + Calcarius_ornatus_18 + Passer_montanus_18 + Pycnonotus_cafer_18 + Sicalis_flaveola_18 + Estrilda_astrild_18 + Pycnonotus_jocosus_18 + Copsychus_malabaricus_18 + Rhynchophanes_mccownii_18 + Paroaria_capitata_18 + Leiothrix_lutea_18 + Peucedramus_taeniatus_18 + Lonchura_atricapilla_18 + Centronyx_bairdii_18 + Chasiempis_sandwichensis_18 + Euodice_cantans_18 + Vidua_macroura_18 + Horornis_diphone_18 + Garrulax_canorus_18 + Calcarius_pictus_18 + Zosterops_simplex_18 + Alauda_arvensis_18 + Oenanthe_oenanthe_18 + Euplectes_franciscanus_18 + Phylloscopus_borealis_18 + Amandava_amandava_18 + Chasiempis_sclateri_18 + Sporophila_torqueola_18 + Luscinia_svecica_18 + Estrilda_melpoda_18 + Chasiempis_ibidis_18 + Pachyramphus_aglaiae_18 + Plectrophenax_hyperboreus_18 + Prunella_montanella_18 + Spindalis_zena_18 + Tarsiger_cyanurus_18 + Coereba_flaveola_18 + Tiaris_olivaceus_18 + Phylloscopus_fuscatus_18 + Lanius_collurio_18 + Muscicapa_griseisticta_18 + Phylloscopus_inornatus_18 + Lanius_cristatus_18 + Acrocephalus_familiaris_18 + Phylloscopus_trochilus_18 + Muscicapa_sibirica_18 + Phylloscopus_collybita_18 + Ficedula_albicilla_18 + Estrilda_troglodytes_18 + Lonchura_malacca_18 + Saxicola_maurus_18 + Larvivora_sibilans_18 + Erithacus_rubecula_18 + Phylloscopus_examinandus_18 + Locustella_lanceolata_18 + Acrocephalus_dumetorum_18 + Cyanerpes_cyaneus_18 + Acrocephalus_schoenobaenus_18 + Luscinia_cyane_18 + Oenanthe_pleschanka_18 + Uraeginthus_bengalus_18 + Phoenicurus_phoenicurus_18 + Pachyramphus_major_18 + Taeniopygia_guttata_18 + Locustella_fluviatilis_18'\n",
    "    \n",
    "for i in range(0, len(c92_copy2.columns[:len(c92_copy2.columns)-1])):\n",
    "    c92_copy2.rename(columns={c92_copy2.columns[i]: lemon[i]}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6df08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "c92_copy2 = c92_copy2.fillna(0)\n",
    "c92_copy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965fee78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1차 종다회귀분석\n",
    "res = ols('target ~ ' + rime, data=c92_copy2).fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72287c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "green_moss = pd.DataFrame(res.pvalues)\n",
    "green_moss\n",
    "green_moss = green_moss[green_moss[0] < 0.05]\n",
    "green_moss.drop(['Intercept'], inplace=True)\n",
    "add_p = list(green_moss.index)\n",
    "\n",
    "\n",
    "lemon2 = ' + '.join(add_p)\n",
    "lemon2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753bf9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2차 종다회귀분석\n",
    "\n",
    "res = ols('target ~ ' + lemon2, data=c92_copy2).fit()\n",
    "res.summary()\n",
    "\n",
    "green_moss = pd.DataFrame(res.pvalues)\n",
    "green_moss = green_moss[green_moss[0] < 0.05]\n",
    "green_moss.drop(['Intercept'], inplace=True)\n",
    "\n",
    "add_p2 = list(green_moss.index)\n",
    "add_p2\n",
    "\n",
    "lemon2 = ' + '.join(add_p2)\n",
    "lemon2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d973b027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한 번 더\n",
    "res = ols('target ~ ' + lemon2, data=c92_copy2).fit()\n",
    "res.summary()\n",
    "\n",
    "green_moss = pd.DataFrame(res.pvalues)\n",
    "green_moss = green_moss[green_moss[0] < 0.05]\n",
    "green_moss.drop(['Intercept'], inplace=True)\n",
    "\n",
    "add_p2 = list(green_moss.index)\n",
    "add_p2\n",
    "\n",
    "lemon2 = ' + '.join(add_p2)\n",
    "lemon2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74da0e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한 번 더\n",
    "res = ols('target ~ ' + lemon2, data=c92_copy2).fit()\n",
    "res.summary()\n",
    "\n",
    "green_moss = pd.DataFrame(res.pvalues)\n",
    "green_moss = green_moss[green_moss[0] < 0.05]\n",
    "green_moss.drop(['Intercept'], inplace=True)\n",
    "\n",
    "add_p2 = list(green_moss.index)\n",
    "add_p2\n",
    "\n",
    "lemon2 = ' + '.join(add_p2)\n",
    "lemon2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b57b7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한 번 더\n",
    "res = ols('target ~ ' + lemon2, data=c92_copy2).fit()\n",
    "res.summary()\n",
    "\n",
    "green_moss = pd.DataFrame(res.pvalues)\n",
    "green_moss = green_moss[green_moss[0] < 0.05]\n",
    "green_moss.drop(['Intercept'], inplace=True)\n",
    "\n",
    "add_p2 = list(green_moss.index)\n",
    "add_p2\n",
    "\n",
    "lemon2 = ' + '.join(add_p2)\n",
    "lemon2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64488269",
   "metadata": {},
   "source": [
    "#### 결과 종합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed4ab45",
   "metadata": {},
   "outputs": [],
   "source": [
    "c92 = c92.drop(columns = 'target')\n",
    "c92 = c92.drop(columns = 'Species_18')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a20cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 각 C9 gps 인근에서의 보고 건수(중복은 누락)\n",
    "\n",
    "# lemon = []\n",
    "# for i in c92.columns:\n",
    "#     lemon.append(i.replace(' ','_'))\n",
    "\n",
    "# for i in range(0, len(lemon)):\n",
    "#     c92.rename(columns={c92.columns[i]: lemon[i]}, inplace = True)\n",
    "    \n",
    "# dicts1 = {}\n",
    "# for i in lemon:\n",
    "#     dicts1[i] = c92[i].sum()\n",
    "    \n",
    "# dicts1 = sorted(dicts1.items(), key = lambda item: item[1], reverse = True)\n",
    "# dicts1\n",
    "\n",
    "\n",
    "\n",
    "## 각 C9 gps 인근에 있었는지 여부에 대한 체크\n",
    "\n",
    "dicts2 = {}\n",
    "for i in c92.columns:\n",
    "    dicts2[i] = len(c92[c92[i] != 0])\n",
    "    \n",
    "dicts2 = sorted(dicts2.items(), key = lambda item: item[1], reverse = True)\n",
    "dicts2\n",
    "dicts2 = dicts2[:4]\n",
    "add_f = [x[0] for x in dicts2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0da5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_f2 = []\n",
    "\n",
    "for i in add_f:\n",
    "    add_f2.append(i.replace(' ', '_'))\n",
    "    \n",
    "add_f = add_f2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fa5ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "MC = list(set(add_p1 + add_f))\n",
    "DJ = list(set(add_p2 + add_f))\n",
    "\n",
    "len(MC)\n",
    "len(DJ)\n",
    "\n",
    "len(MC)\n",
    "len(DJ)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e067396",
   "metadata": {},
   "outputs": [],
   "source": [
    "MC\n",
    "DJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad811790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# c92_copy2['risk_degree2'] = c92_backup['risk_degree2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45dc9919",
   "metadata": {},
   "outputs": [],
   "source": [
    "c92sop1_x = c92_copy2[MC]\n",
    "c92sop2_x = c92_copy2[DJ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf147c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# c92sop1_x = c92sr2_x[['Coccinella septempunctata', 'Harmonia axyridis', 'Cryptolaemus montrouzieri', 'Axion plagiatum', 'Coccinella transversoguttata', 'Hippodamia convergens', 'Adalia bipunctata', 'Cycloneda sanguinea', 'Coccinella californica', 'Hippodamia parenthesis', 'Hyperaspis lateralis']]\n",
    "\n",
    "# c92sop2_x = c92sr2_x[['Cryptolaemus montrouzieri', 'Axion plagiatum', 'Coccinella transversoguttata', 'Hippodamia convergens', 'Adalia bipunctata', 'Cycloneda sanguinea', 'Coccinella californica', 'Hippodamia parenthesis', 'Hyperaspis lateralis']]\n",
    "\n",
    "# c92sopr_x = c92sr2_x[['Cryptolaemus montrouzieri', 'Axion plagiatum', 'Coccinella transversoguttata', 'Hippodamia convergens', 'Adalia bipunctata', 'Cycloneda sanguinea', 'Coccinella californica', 'Hippodamia parenthesis', 'Hyperaspis lateralis', 'risk_degree2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4560d910",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e42ecc2f",
   "metadata": {},
   "source": [
    "#### 다중공산성\n",
    "- 10 이하, 엄격할 경우 5 이하"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5be0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = sns.light_palette(\"darkgray\", as_cmap=True)\n",
    "sns.heatmap(c92sop1_x.corr(), annot=True, cmap=cmap)\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.show()\n",
    "\n",
    "\n",
    "cmap = sns.light_palette(\"darkgray\", as_cmap=True)\n",
    "sns.heatmap(c92sop2_x.corr(), annot=True, cmap=cmap)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e317a45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "c92_copy2[MC]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b36349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Axion plagiatum~@@@Hippodamia convergens~Cycloneda sanguinea\n",
    "# @@@Hippodamia parenthesis\n",
    "# Coccinella transversoguttata\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "\n",
    "# 골라서 넣기\n",
    "X = c92_copy2[MC]\n",
    "#X = c92_copy2[DJ]\n",
    "\n",
    "\n",
    "X.columns\n",
    "\n",
    "vif = pd.DataFrame()\n",
    "vif[\"VIF Factor\"] = [variance_inflation_factor(\n",
    "    X.values, i) for i in range(X.shape[1])]\n",
    "vif[\"features\"] = X.columns\n",
    "vif\n",
    "\n",
    "try:\n",
    "    vif.loc[vif['VIF Factor'] < 10]\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "# Cryptolaemus montrouzieri\n",
    "# Axion plagiatum~@@@Hippodamia convergens~Cycloneda sanguinea\n",
    "\n",
    "# @@@Hippodamia parenthesis\n",
    "## Coccinella transversoguttata\n",
    "\n",
    "\n",
    "# sangui ~ C7#, conver, plagiatum, axyridis\n",
    "# C7 ~ sang, conv, axy, plagia\n",
    "\n",
    "# tran ~ parenthesis # paren ~ transvers,\n",
    "\n",
    "# conv ~ sang, c7, plagiatum, axyridis \n",
    "# axion, sangui, c7, conv, axy\n",
    "# axy ~ sang, c7, conv, axion, \n",
    "\n",
    "# lat ~ punctill, scymus caurinus\n",
    "# punc ~ lat, caurinus\n",
    "# scymnus ~ transvers, lateralis, punctillum\n",
    "\n",
    "\n",
    "# Mc의 살릴 목록\n",
    "# Hippodamia_convergens\n",
    "# Hippodamia_parenthesis\n",
    "# Scymnus_caurinus\n",
    "\n",
    "\n",
    "\n",
    "# ----------\n",
    "\n",
    "# MCr 목록\n",
    "# Sangui ~ cone, plaㅎ\n",
    "# Trans ~ parenthesis\n",
    "# Lat - ounce, cairn\n",
    "# Punc - lat, caurin\n",
    "# Cons - Sangui, plag\n",
    "# Parent - trans, \n",
    "# Axion - Sangui, cone, \n",
    "# Cairn - lat, punc\n",
    "\n",
    "for i in vif.index:\n",
    "    if vif['VIF Factor'][i] > 10:\n",
    "        vif.drop(index = i)\n",
    "        \n",
    "geek = vif.features.tolist()\n",
    "MC = geek.copy()\n",
    "#DJ = geek.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78330529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Axion plagiatum~@@@Hippodamia convergens~Cycloneda sanguinea\n",
    "# @@@Hippodamia parenthesis\n",
    "# Coccinella transversoguttata\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "\n",
    "# 골라서 넣기\n",
    "# X = c92_copy2[MC]\n",
    "X = c92_copy2[DJ]\n",
    "\n",
    "\n",
    "X.columns\n",
    "\n",
    "vif = pd.DataFrame()\n",
    "vif[\"VIF Factor\"] = [variance_inflation_factor(\n",
    "    X.values, i) for i in range(X.shape[1])]\n",
    "vif[\"features\"] = X.columns\n",
    "vif\n",
    "\n",
    "try:\n",
    "    vif.loc[vif['VIF Factor'] < 10]\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "# Cryptolaemus montrouzieri\n",
    "# Axion plagiatum~@@@Hippodamia convergens~Cycloneda sanguinea\n",
    "\n",
    "# @@@Hippodamia parenthesis\n",
    "## Coccinella transversoguttata\n",
    "\n",
    "\n",
    "# sangui ~ C7#, conver, plagiatum, axyridis\n",
    "# C7 ~ sang, conv, axy, plagia\n",
    "\n",
    "# tran ~ parenthesis # paren ~ transvers,\n",
    "\n",
    "# conv ~ sang, c7, plagiatum, axyridis \n",
    "# axion, sangui, c7, conv, axy\n",
    "# axy ~ sang, c7, conv, axion, \n",
    "\n",
    "# lat ~ punctill, scymus caurinus\n",
    "# punc ~ lat, caurinus\n",
    "# scymnus ~ transvers, lateralis, punctillum\n",
    "\n",
    "\n",
    "# Mc의 살릴 목록\n",
    "# Hippodamia_convergens\n",
    "# Hippodamia_parenthesis\n",
    "# Scymnus_caurinus\n",
    "\n",
    "\n",
    "\n",
    "# ----------\n",
    "\n",
    "# MCr 목록\n",
    "# Sangui ~ cone, plaㅎ\n",
    "# Trans ~ parenthesis\n",
    "# Lat - ounce, cairn\n",
    "# Punc - lat, caurin\n",
    "# Cons - Sangui, plag\n",
    "# Parent - trans, \n",
    "# Axion - Sangui, cone, \n",
    "# Cairn - lat, punc\n",
    "\n",
    "for i in vif.index:\n",
    "    if vif['VIF Factor'][i] > 10:\n",
    "        vif.drop(index = i)\n",
    "        \n",
    "geek = vif.features.tolist()\n",
    "#MC = geek.copy()\n",
    "DJ = geek.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38849480",
   "metadata": {},
   "source": [
    "## 머신러닝"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94a430f",
   "metadata": {},
   "source": [
    "### 압축 전(변수 10개)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c244ec0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "c92_copy2[MC]\n",
    "KVIM = c92_copy2[DJ]\n",
    "# KVIM.drop(columns = ['Lonchura_cucullata_1', 'Locustella_ochotensis_1'], inplace=True)\n",
    "\n",
    "BBOY=['Harmonia_axyridis_18', 'Coccinella_septempunctata_18', 'Hippodamia_convergens_18', 'Eremophila_alpestris_18']\n",
    "\n",
    "TAGGER1 = ['Anatis_labiculata_18', 'Hippodamia_apicalis_18', 'Microweisea_misella_18', 'Delphastus_pusillus_18', 'Amphispiza_bilineata_18']\n",
    "\n",
    "TAGGER2 = list(set(TAGGER1 + DJ))\n",
    "\n",
    "TAGGER3 = ['Delphastus_pusillus_18',\n",
    " 'Plectrophenax_nivalis_18',\n",
    " 'Hippodamia_convergens_18',\n",
    " 'Pipilo_chlorurus_18',\n",
    " 'Artemisiospiza_belli_18',\n",
    " 'Phainopepla_nitens_18',\n",
    " 'Myzia_interrupta_18',\n",
    " 'Hippodamia_quinquesignata_18',\n",
    " 'Eremophila_alpestris_18',\n",
    " 'Coccinella_transversoguttata_18',\n",
    " 'Hippodamia_caseyi_18',\n",
    " 'Harmonia_axyridis_18',\n",
    " 'Coccinella_monticola_18',\n",
    " 'Subcoccinella_vigintiquatuorpunctata_18',\n",
    " 'Melozone_crissalis_18',\n",
    " 'Melozone_aberti_18',\n",
    " 'Microweisea_misella_18',\n",
    " 'Hyperaspis_quadrioculata_18',\n",
    " 'Amphispiza_bilineata_18',\n",
    " 'Coccinella_septempunctata_18',\n",
    " 'Hippodamia_glacialis_18',\n",
    " 'Exochomus_aethiops_18',\n",
    " 'Lonchura_punctulata_18']\n",
    "\n",
    "\n",
    "c92_copy2[BBOY]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ab8f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "DJ\n",
    "MC\n",
    "BBOY\n",
    "TAGGER2\n",
    "\n",
    "\n",
    "len(MC)\n",
    "len(DJ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadbe6ef",
   "metadata": {},
   "source": [
    "################ 임시로 해놓은 필드임. 추후 꼭 삭제하기. 아니면 변수 구성이 고정되어버림.\n",
    "\n",
    "\n",
    "\n",
    "TAGGER = ['Coelophora_maculata_18',\n",
    " 'Coccinella_californica_18',\n",
    " 'Adalia_bipunctata_18',\n",
    " 'Eremophila_alpestris_18',\n",
    " 'Zonotrichia_leucophrys_18',\n",
    " 'Spizella_passerina_18']\n",
    "\n",
    "\n",
    "MC = ['Adalia_bipunctata_18',\n",
    " 'Coccinella_californica_18',\n",
    " 'Harmonia_axyridis_18',\n",
    " 'Zonotrichia_leucophrys_18',\n",
    " 'Hippodamia_convergens_18',\n",
    " 'Coccinella_septempunctata_18',\n",
    " 'Spizella_passerina_18',\n",
    " 'Coelophora_maculata_18',\n",
    " 'Eremophila_alpestris_18']\n",
    "\n",
    "DJ = ['Pipilo_chlorurus_18',\n",
    " 'Coccinella_transversoguttata_18',\n",
    " 'Hippodamia_convergens_18',\n",
    " 'Lonchura_punctulata_18',\n",
    " 'Plectrophenax_nivalis_18',\n",
    " 'Exochomus_aethiops_18',\n",
    " 'Hippodamia_apicalis_18',\n",
    " 'Hippodamia_glacialis_18',\n",
    " 'Artemisiospiza_belli_18',\n",
    " 'Harmonia_axyridis_18',\n",
    " 'Subcoccinella_vigintiquatuorpunctata_18',\n",
    " 'Phainopepla_nitens_18',\n",
    " 'Coccinella_monticola_18',\n",
    " 'Myzia_interrupta_18',\n",
    " 'Melozone_crissalis_18',\n",
    " 'Coccinella_septempunctata_18',\n",
    " 'Melozone_aberti_18',\n",
    " 'Eremophila_alpestris_18',\n",
    " 'Hippodamia_quinquesignata_18',\n",
    " 'Zonotrichia_leucophrys_18',\n",
    " 'Hyperaspis_quadrioculata_18',\n",
    " 'Hippodamia_caseyi_18']\n",
    "\n",
    "\n",
    "BBOY=['Harmonia_axyridis_18', 'Coccinella_septempunctata_18', 'Coelophora_maculata_18', 'Hippodamia_convergens_18', 'Coccinella_transversoguttata_18', 'Eremophila_alpestris_18', 'Zonotrichia_leucophrys_18']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b140adbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "c92_copy2.Species_18.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e8a723",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in c92_copy2.index:\n",
    "    if c92_copy2.target[i] != 1:\n",
    "        print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0015e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "c92_copy2.Species_18.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368c0de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "c92_copy2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd6db2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "BEATMKR = ['Harmonia_axyridis_18', 'Coccinella_septempunctata_18',\n",
    "       'Hippodamia_convergens_18', 'Coelophora_maculata_18',\n",
    "       'Cycloneda_sanguinea_18', 'Propylea_quatuordecimpunctata_18',\n",
    "       'Cycloneda_munda_18', 'Psyllobora_vigintimaculata_18',\n",
    "       'Chilocorus_stigma_18', 'Olla_vnigrum_18', 'Hippodamia_variegata_18',\n",
    "       'Adalia_bipunctata_18', 'Coccinella_californica_18',\n",
    "       'Cycloneda_polita_18', 'Cryptolaemus_montrouzieri_18',\n",
    "       'Coccinella_trifasciata_18', 'Epilachna_borealis_18', 'Anatis_mali_18',\n",
    "       'Chilocorus_cacti_18', 'Brachiacantha_ursina_18',\n",
    "       'Hippodamia_parenthesis_18', 'Anatis_labiculata_18',\n",
    "       'Mulsantina_picta_18', 'Coccinella_transversoguttata_18',\n",
    "       'Psyllobora_renifer_18', 'Naemia_seriata_18',\n",
    "       'Hippodamia_tredecimpunctata_18', 'Epilachna_varivestis_18',\n",
    "       'Hippodamia_glacialis_18', 'Anatis_lecontei_18', 'Axion_plagiatum_18',\n",
    "       'Calvia_quatuordecimguttata_18', 'Coleomegilla_maculata_18',\n",
    "       'Stethorus_punctillum_18', 'Anatis_rathvoni_18',\n",
    "       'Psyllobora_borealis_18', 'Curinus_coeruleus_18', 'Myzia_pullata_18',\n",
    "       'Halmus_chalybeus_18', 'Neoharmonia_venusta_18',\n",
    "       'Exochomus_childreni_18', 'Hippodamia_caseyi_18',\n",
    "       'Coelophora_inaequalis_18', 'Coccinella_monticola_18',\n",
    "       'Mulsantina_hudsonica_18', 'Diomus_terminatus_18',\n",
    "       'Anisosticta_bitriangularis_18', 'Hyperaspis_binotata_18',\n",
    "       'Exochomus_quadripustulatus_18', 'Hippodamia_sinuata_18',\n",
    "       'Brumoides_septentrionis_18', 'Hippodamia_quinquesignata_18',\n",
    "       'Hyperaspis_bigeminata_18', 'Paranaemia_vittigera_18',\n",
    "       'Chilocorus_bipustulatus_18', 'Rhyzobius_lophanthae_18',\n",
    "       'Novius_cardinalis_18', 'Scymnus_loewii_18',\n",
    "       'Brachiacantha_decempustulata_18', 'Scymnus_nebulosus_18',\n",
    "       'Didion_punctatum_18', 'Stethorus_punctum_18', 'Myzia_interrupta_18',\n",
    "       'Hyperaspis_trifurcata_18', 'Axion_tripustulatum_18',\n",
    "       'Epilachna_tredecimnotata_18',\n",
    "       'Subcoccinella_vigintiquatuorpunctata_18', 'Hippodamia_apicalis_18',\n",
    "       'Hyperaspis_undulata_18', 'Exochomus_aethiops_18',\n",
    "       'Hyperaspis_lateralis_18', 'Coccinella_hieroglyphica_18',\n",
    "       'Hyperaspis_quadrioculata_18', 'Microweisea_misella_18',\n",
    "       'Exochomus_marginipennis_18', 'Hippodamia_oregonensis_18',\n",
    "       'Brachiacantha_dentipes_18', 'Myzia_subvittata_18',\n",
    "       'Psyllobora_parvinotata_18', 'Nephus_flavifrons_18',\n",
    "       'Hyperaspis_proba_18', 'Brachiacantha_quadripunctata_18',\n",
    "       'Egius_platycephalus_18', 'Hyperaspis_connectens_18',\n",
    "       'Nephus_intrusus_18', 'Scymnus_louisianae_18', 'Exochomus_fasciatus_18',\n",
    "       'Coccinella_undecimpunctata_18', 'Delphastus_pusillus_18',\n",
    "       'Eremophila_alpestris_18', 'Lanius_ludovicianus_18',\n",
    "       'Plectrophenax_nivalis_18', 'Phainopepla_nitens_18',\n",
    "       'Lanius_borealis_18', 'Calcarius_lapponicus_18', 'Cinclus_mexicanus_18',\n",
    "       'Lonchura_punctulata_18', 'Zosterops_japonicus_18',\n",
    "       'Centronyx_henslowii_18', 'Paroaria_coronata_18',\n",
    "       'Calcarius_ornatus_18', 'Passer_montanus_18', 'Pycnonotus_cafer_18',\n",
    "       'Sicalis_flaveola_18', 'Estrilda_astrild_18', 'Pycnonotus_jocosus_18',\n",
    "       'Copsychus_malabaricus_18', 'Rhynchophanes_mccownii_18',\n",
    "       'Paroaria_capitata_18', 'Leiothrix_lutea_18',\n",
    "       'Peucedramus_taeniatus_18', 'Lonchura_atricapilla_18',\n",
    "       'Centronyx_bairdii_18', 'Chasiempis_sandwichensis_18',\n",
    "       'Euodice_cantans_18', 'Vidua_macroura_18', 'Horornis_diphone_18',\n",
    "       'Garrulax_canorus_18', 'Calcarius_pictus_18', 'Zosterops_simplex_18',\n",
    "       'Alauda_arvensis_18', 'Oenanthe_oenanthe_18',\n",
    "       'Euplectes_franciscanus_18', 'Phylloscopus_borealis_18',\n",
    "       'Amandava_amandava_18', 'Chasiempis_sclateri_18',\n",
    "       'Sporophila_torqueola_18', 'Luscinia_svecica_18', 'Estrilda_melpoda_18',\n",
    "       'Chasiempis_ibidis_18', 'Pachyramphus_aglaiae_18',\n",
    "       'Plectrophenax_hyperboreus_18', 'Prunella_montanella_18',\n",
    "       'Spindalis_zena_18', 'Tarsiger_cyanurus_18', 'Coereba_flaveola_18',\n",
    "       'Tiaris_olivaceus_18', 'Phylloscopus_fuscatus_18', 'Lanius_collurio_18',\n",
    "       'Muscicapa_griseisticta_18', 'Phylloscopus_inornatus_18',\n",
    "       'Lanius_cristatus_18', 'Acrocephalus_familiaris_18',\n",
    "       'Phylloscopus_trochilus_18', 'Muscicapa_sibirica_18',\n",
    "       'Ficedula_albicilla_18', 'Phylloscopus_collybita_18',\n",
    "       'Estrilda_troglodytes_18', 'Lonchura_malacca_18', 'Saxicola_maurus_18',\n",
    "       'Erithacus_rubecula_18', 'Phylloscopus_examinandus_18',\n",
    "       'Larvivora_sibilans_18', 'Locustella_lanceolata_18',\n",
    "       'Cyanerpes_cyaneus_18', 'Acrocephalus_schoenobaenus_18',\n",
    "       'Acrocephalus_dumetorum_18', 'Oenanthe_pleschanka_18',\n",
    "       'Locustella_fluviatilis_18', 'Phoenicurus_phoenicurus_18',\n",
    "       'Uraeginthus_bengalus_18', 'Pachyramphus_major_18', 'Luscinia_cyane_18',\n",
    "       'Taeniopygia_guttata_18']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "BEATMKR_CE = ['Harmonia_axyridis_18', 'Coccinella_septempunctata_18',\n",
    "    'Coelophora_maculata_18', # 'Hippodamia_convergens_18',\n",
    "       'Cycloneda_sanguinea_18', 'Propylea_quatuordecimpunctata_18',\n",
    "       'Cycloneda_munda_18', 'Psyllobora_vigintimaculata_18',\n",
    "       'Chilocorus_stigma_18', 'Olla_vnigrum_18', 'Hippodamia_variegata_18',\n",
    "       'Adalia_bipunctata_18', 'Coccinella_californica_18',\n",
    "       'Cycloneda_polita_18', 'Cryptolaemus_montrouzieri_18',\n",
    "       'Coccinella_trifasciata_18', 'Epilachna_borealis_18', 'Anatis_mali_18',\n",
    "       'Chilocorus_cacti_18', 'Brachiacantha_ursina_18',\n",
    "       'Hippodamia_parenthesis_18', 'Anatis_labiculata_18',\n",
    "       'Mulsantina_picta_18', 'Coccinella_transversoguttata_18',\n",
    "       'Psyllobora_renifer_18', 'Naemia_seriata_18',\n",
    "       'Hippodamia_tredecimpunctata_18', 'Epilachna_varivestis_18',\n",
    "       'Hippodamia_glacialis_18', 'Anatis_lecontei_18', 'Axion_plagiatum_18',\n",
    "       'Calvia_quatuordecimguttata_18', 'Coleomegilla_maculata_18',\n",
    "       'Stethorus_punctillum_18', 'Anatis_rathvoni_18',\n",
    "       'Psyllobora_borealis_18', 'Curinus_coeruleus_18', 'Myzia_pullata_18',\n",
    "       'Halmus_chalybeus_18', 'Neoharmonia_venusta_18',\n",
    "       'Exochomus_childreni_18', 'Hippodamia_caseyi_18',\n",
    "       'Coelophora_inaequalis_18', 'Coccinella_monticola_18',\n",
    "       'Mulsantina_hudsonica_18', 'Diomus_terminatus_18',\n",
    "       'Anisosticta_bitriangularis_18', 'Hyperaspis_binotata_18',\n",
    "       'Exochomus_quadripustulatus_18', 'Hippodamia_sinuata_18',\n",
    "       'Brumoides_septentrionis_18', 'Hippodamia_quinquesignata_18',\n",
    "       'Hyperaspis_bigeminata_18', 'Paranaemia_vittigera_18',\n",
    "       'Chilocorus_bipustulatus_18', 'Rhyzobius_lophanthae_18',\n",
    "       'Novius_cardinalis_18', 'Scymnus_loewii_18',\n",
    "       'Brachiacantha_decempustulata_18', 'Scymnus_nebulosus_18',\n",
    "       'Didion_punctatum_18', 'Stethorus_punctum_18', 'Myzia_interrupta_18',\n",
    "       'Hyperaspis_trifurcata_18', 'Axion_tripustulatum_18',\n",
    "       'Epilachna_tredecimnotata_18',\n",
    "       'Subcoccinella_vigintiquatuorpunctata_18', 'Hippodamia_apicalis_18',\n",
    "       'Hyperaspis_undulata_18', 'Exochomus_aethiops_18',\n",
    "       'Hyperaspis_lateralis_18', 'Coccinella_hieroglyphica_18',\n",
    "       'Hyperaspis_quadrioculata_18', 'Microweisea_misella_18',\n",
    "       'Exochomus_marginipennis_18', 'Hippodamia_oregonensis_18',\n",
    "       'Brachiacantha_dentipes_18', 'Myzia_subvittata_18',\n",
    "       'Psyllobora_parvinotata_18', 'Nephus_flavifrons_18',\n",
    "       'Hyperaspis_proba_18', 'Brachiacantha_quadripunctata_18',\n",
    "       'Egius_platycephalus_18', 'Hyperaspis_connectens_18',\n",
    "       'Nephus_intrusus_18', 'Scymnus_louisianae_18', 'Exochomus_fasciatus_18',\n",
    "       'Coccinella_undecimpunctata_18', 'Delphastus_pusillus_18',\n",
    "       'Eremophila_alpestris_18', 'Lanius_ludovicianus_18',\n",
    "       'Plectrophenax_nivalis_18', 'Phainopepla_nitens_18',\n",
    "       'Lanius_borealis_18', 'Calcarius_lapponicus_18', 'Cinclus_mexicanus_18',\n",
    "       'Lonchura_punctulata_18', 'Zosterops_japonicus_18',\n",
    "       'Centronyx_henslowii_18', 'Paroaria_coronata_18',\n",
    "       'Calcarius_ornatus_18', 'Passer_montanus_18', 'Pycnonotus_cafer_18',\n",
    "       'Sicalis_flaveola_18', 'Estrilda_astrild_18', 'Pycnonotus_jocosus_18',\n",
    "       'Copsychus_malabaricus_18', 'Rhynchophanes_mccownii_18',\n",
    "       'Paroaria_capitata_18', 'Leiothrix_lutea_18',\n",
    "       'Peucedramus_taeniatus_18', 'Lonchura_atricapilla_18',\n",
    "       'Centronyx_bairdii_18', 'Chasiempis_sandwichensis_18',\n",
    "       'Euodice_cantans_18', 'Vidua_macroura_18', 'Horornis_diphone_18',\n",
    "       'Garrulax_canorus_18', 'Calcarius_pictus_18', 'Zosterops_simplex_18',\n",
    "       'Alauda_arvensis_18', 'Oenanthe_oenanthe_18',\n",
    "       'Euplectes_franciscanus_18', 'Phylloscopus_borealis_18',\n",
    "       'Amandava_amandava_18', 'Chasiempis_sclateri_18',\n",
    "       'Sporophila_torqueola_18', 'Luscinia_svecica_18', 'Estrilda_melpoda_18',\n",
    "       'Chasiempis_ibidis_18', 'Pachyramphus_aglaiae_18',\n",
    "       'Plectrophenax_hyperboreus_18', 'Prunella_montanella_18',\n",
    "       'Spindalis_zena_18', 'Tarsiger_cyanurus_18', 'Coereba_flaveola_18',\n",
    "       'Tiaris_olivaceus_18', 'Phylloscopus_fuscatus_18', 'Lanius_collurio_18',\n",
    "       'Muscicapa_griseisticta_18', 'Phylloscopus_inornatus_18',\n",
    "       'Lanius_cristatus_18', 'Acrocephalus_familiaris_18',\n",
    "       'Phylloscopus_trochilus_18', 'Muscicapa_sibirica_18',\n",
    "       'Ficedula_albicilla_18', 'Phylloscopus_collybita_18',\n",
    "       'Estrilda_troglodytes_18', 'Lonchura_malacca_18', 'Saxicola_maurus_18',\n",
    "       'Erithacus_rubecula_18', 'Phylloscopus_examinandus_18',\n",
    "       'Larvivora_sibilans_18', 'Locustella_lanceolata_18',\n",
    "       'Cyanerpes_cyaneus_18', 'Acrocephalus_schoenobaenus_18',\n",
    "       'Acrocephalus_dumetorum_18', 'Oenanthe_pleschanka_18',\n",
    "       'Locustella_fluviatilis_18', 'Phoenicurus_phoenicurus_18',\n",
    "       'Uraeginthus_bengalus_18', 'Pachyramphus_major_18', 'Luscinia_cyane_18',\n",
    "       'Taeniopygia_guttata_18']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "BEATMKR_SE = ['Harmonia_axyridis_18', # 'Coccinella_septempunctata_18',\n",
    "       'Hippodamia_convergens_18', 'Coelophora_maculata_18',\n",
    "       'Cycloneda_sanguinea_18', 'Propylea_quatuordecimpunctata_18',\n",
    "       'Cycloneda_munda_18', 'Psyllobora_vigintimaculata_18',\n",
    "       'Chilocorus_stigma_18', 'Olla_vnigrum_18', 'Hippodamia_variegata_18',\n",
    "       'Adalia_bipunctata_18', 'Coccinella_californica_18',\n",
    "       'Cycloneda_polita_18', 'Cryptolaemus_montrouzieri_18',\n",
    "       'Coccinella_trifasciata_18', 'Epilachna_borealis_18', 'Anatis_mali_18',\n",
    "       'Chilocorus_cacti_18', 'Brachiacantha_ursina_18',\n",
    "       'Hippodamia_parenthesis_18', 'Anatis_labiculata_18',\n",
    "       'Mulsantina_picta_18', 'Coccinella_transversoguttata_18',\n",
    "       'Psyllobora_renifer_18', 'Naemia_seriata_18',\n",
    "       'Hippodamia_tredecimpunctata_18', 'Epilachna_varivestis_18',\n",
    "       'Hippodamia_glacialis_18', 'Anatis_lecontei_18', 'Axion_plagiatum_18',\n",
    "       'Calvia_quatuordecimguttata_18', 'Coleomegilla_maculata_18',\n",
    "       'Stethorus_punctillum_18', 'Anatis_rathvoni_18',\n",
    "       'Psyllobora_borealis_18', 'Curinus_coeruleus_18', 'Myzia_pullata_18',\n",
    "       'Halmus_chalybeus_18', 'Neoharmonia_venusta_18',\n",
    "       'Exochomus_childreni_18', 'Hippodamia_caseyi_18',\n",
    "       'Coelophora_inaequalis_18', 'Coccinella_monticola_18',\n",
    "       'Mulsantina_hudsonica_18', 'Diomus_terminatus_18',\n",
    "       'Anisosticta_bitriangularis_18', 'Hyperaspis_binotata_18',\n",
    "       'Exochomus_quadripustulatus_18', 'Hippodamia_sinuata_18',\n",
    "       'Brumoides_septentrionis_18', 'Hippodamia_quinquesignata_18',\n",
    "       'Hyperaspis_bigeminata_18', 'Paranaemia_vittigera_18',\n",
    "       'Chilocorus_bipustulatus_18', 'Rhyzobius_lophanthae_18',\n",
    "       'Novius_cardinalis_18', 'Scymnus_loewii_18',\n",
    "       'Brachiacantha_decempustulata_18', 'Scymnus_nebulosus_18',\n",
    "       'Didion_punctatum_18', 'Stethorus_punctum_18', 'Myzia_interrupta_18',\n",
    "       'Hyperaspis_trifurcata_18', 'Axion_tripustulatum_18',\n",
    "       'Epilachna_tredecimnotata_18',\n",
    "       'Subcoccinella_vigintiquatuorpunctata_18', 'Hippodamia_apicalis_18',\n",
    "       'Hyperaspis_undulata_18', 'Exochomus_aethiops_18',\n",
    "       'Hyperaspis_lateralis_18', 'Coccinella_hieroglyphica_18',\n",
    "       'Hyperaspis_quadrioculata_18', 'Microweisea_misella_18',\n",
    "       'Exochomus_marginipennis_18', 'Hippodamia_oregonensis_18',\n",
    "       'Brachiacantha_dentipes_18', 'Myzia_subvittata_18',\n",
    "       'Psyllobora_parvinotata_18', 'Nephus_flavifrons_18',\n",
    "       'Hyperaspis_proba_18', 'Brachiacantha_quadripunctata_18',\n",
    "       'Egius_platycephalus_18', 'Hyperaspis_connectens_18',\n",
    "       'Nephus_intrusus_18', 'Scymnus_louisianae_18', 'Exochomus_fasciatus_18',\n",
    "       'Coccinella_undecimpunctata_18', 'Delphastus_pusillus_18',\n",
    "       'Eremophila_alpestris_18', 'Lanius_ludovicianus_18',\n",
    "       'Plectrophenax_nivalis_18', 'Phainopepla_nitens_18',\n",
    "       'Lanius_borealis_18', 'Calcarius_lapponicus_18', 'Cinclus_mexicanus_18',\n",
    "       'Lonchura_punctulata_18', 'Zosterops_japonicus_18',\n",
    "       'Centronyx_henslowii_18', 'Paroaria_coronata_18',\n",
    "       'Calcarius_ornatus_18', 'Passer_montanus_18', 'Pycnonotus_cafer_18',\n",
    "       'Sicalis_flaveola_18', 'Estrilda_astrild_18', 'Pycnonotus_jocosus_18',\n",
    "       'Copsychus_malabaricus_18', 'Rhynchophanes_mccownii_18',\n",
    "       'Paroaria_capitata_18', 'Leiothrix_lutea_18',\n",
    "       'Peucedramus_taeniatus_18', 'Lonchura_atricapilla_18',\n",
    "       'Centronyx_bairdii_18', 'Chasiempis_sandwichensis_18',\n",
    "       'Euodice_cantans_18', 'Vidua_macroura_18', 'Horornis_diphone_18',\n",
    "       'Garrulax_canorus_18', 'Calcarius_pictus_18', 'Zosterops_simplex_18',\n",
    "       'Alauda_arvensis_18', 'Oenanthe_oenanthe_18',\n",
    "       'Euplectes_franciscanus_18', 'Phylloscopus_borealis_18',\n",
    "       'Amandava_amandava_18', 'Chasiempis_sclateri_18',\n",
    "       'Sporophila_torqueola_18', 'Luscinia_svecica_18', 'Estrilda_melpoda_18',\n",
    "       'Chasiempis_ibidis_18', 'Pachyramphus_aglaiae_18',\n",
    "       'Plectrophenax_hyperboreus_18', 'Prunella_montanella_18',\n",
    "       'Spindalis_zena_18', 'Tarsiger_cyanurus_18', 'Coereba_flaveola_18',\n",
    "       'Tiaris_olivaceus_18', 'Phylloscopus_fuscatus_18', 'Lanius_collurio_18',\n",
    "       'Muscicapa_griseisticta_18', 'Phylloscopus_inornatus_18',\n",
    "       'Lanius_cristatus_18', 'Acrocephalus_familiaris_18',\n",
    "       'Phylloscopus_trochilus_18', 'Muscicapa_sibirica_18',\n",
    "       'Ficedula_albicilla_18', 'Phylloscopus_collybita_18',\n",
    "       'Estrilda_troglodytes_18', 'Lonchura_malacca_18', 'Saxicola_maurus_18',\n",
    "       'Erithacus_rubecula_18', 'Phylloscopus_examinandus_18',\n",
    "       'Larvivora_sibilans_18', 'Locustella_lanceolata_18',\n",
    "       'Cyanerpes_cyaneus_18', 'Acrocephalus_schoenobaenus_18',\n",
    "       'Acrocephalus_dumetorum_18', 'Oenanthe_pleschanka_18',\n",
    "       'Locustella_fluviatilis_18', 'Phoenicurus_phoenicurus_18',\n",
    "       'Uraeginthus_bengalus_18', 'Pachyramphus_major_18', 'Luscinia_cyane_18',\n",
    "       'Taeniopygia_guttata_18']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce2739c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DJ = list(set(DJ) - set(['Pipilo chlorurus','Melozone crissalis','Exochomus aethiops','Melozone aberti','Artemisiospiza belli','Zonotrichia leucophrys']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef978b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOX = c92_copy2[c92_copy2.Species_18 != 'Hippodamia parenthesis'].reset_index(drop = True)\n",
    "sample = BOX[324:].sample(n=324, random_state = 3)\n",
    "c92_copy22 = pd.concat([BOX[:324], sample])\n",
    "\n",
    "BOX[:324].tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab507041",
   "metadata": {},
   "outputs": [],
   "source": [
    "MC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d257223b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PTYPE = [  \n",
    "# 'Harmonia_axyridis_18',\n",
    "# 'Coccinella_transversoguttata_18',\n",
    "# 'Hippodamia_convergens_18',\n",
    "# 'Eremophila_alpestris_18',\n",
    "# 'Coccinella_septempunctata_18',\n",
    "# 'Hippodamia_parenthesis_18',\n",
    "# 'Phainopepla_nitens_18',\n",
    "# 'Lanius_borealis_18',\n",
    "# 'Calcarius_lapponicus_18',\n",
    "# 'Lonchura_punctulata_18',\n",
    "# 'Exochomus_aethiops_18',\n",
    "# 'Hippodamia_tredecimpunctata_18',\n",
    "# 'Psyllobora_vigintimaculata_18'\n",
    "# ]\n",
    "\n",
    "\n",
    "# PTYPE = [  \n",
    "# 'Phainopepla_nitens_18',\n",
    "# 'Lanius_borealis_18',\n",
    "# 'Lonchura_punctulata_18',\n",
    "# 'Hippodamia_tredecimpunctata_18',\n",
    "# 'Psyllobora_vigintimaculata_18'\n",
    "# ]\n",
    "\n",
    "PTYPE = ['Harmonia_axyridis_18', 'Coelophora_maculata_18',\n",
    "       'Hippodamia_convergens_18', 'Coccinella_transversoguttata_18',\n",
    "       'Coccinella_septempunctata_18', 'Eremophila_alpestris_18',\n",
    "       'Hippodamia_parenthesis_18', 'Lanius_ludovicianus_18',\n",
    "       'Calcarius_lapponicus_18', 'Hippodamia_caseyi_18',\n",
    "       'Calcarius_ornatus_18', 'Hippodamia_glacialis_18',\n",
    "       'Exochomus_aethiops_18', 'Rhynchophanes_mccownii_18',\n",
    "       'Halmus_chalybeus_18']\n",
    "\n",
    "# PTYPE = ['Harmonia_axyridis_18',\n",
    "# 'Coccinella_transversoguttata_18',\n",
    "# 'Hippodamia_convergens_18',\n",
    "# 'Eremophila_alpestris_18',\n",
    "# 'Coccinella_septempunctata_18',\n",
    "# 'Lanius_borealis_18',\n",
    "# 'Phainopepla_nitens_18',\n",
    "# 'Hippodamia_parenthesis_18',\n",
    "# \"Calcarius_lapponicus_18\",\n",
    "# 'Lonchura_punctulata_18',\n",
    "# 'Exochomus_aethiops_18',\n",
    "# 'Hippodamia_caseyi_18',\n",
    "# \"Rhynchophanes_mccownii_18\",\n",
    "# 'Hippodamia_apicalis_18',\n",
    "# 'Calcarius_ornatus_18',\n",
    "# 'Coccinella_monticola_18']  # Absence 3종 + log 버전"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0a3094",
   "metadata": {},
   "source": [
    "## check point!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49e2a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#InteractiveShell.ast_node_interactivity=\"all\"\n",
    "InteractiveShell.ast_node_interactivity=\"last_expr\"\n",
    "\n",
    "from sklearn.metrics import brier_score_loss\n",
    "from sklearn.metrics import cohen_kappa_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d11819",
   "metadata": {},
   "outputs": [],
   "source": [
    "no = int(input('숫자입력: 1 = C. nov. // 2 = C. trans // 3 = A. bipunc'))\n",
    "\n",
    "if no == 1:\n",
    "    target_species = 'Coccinella novemnotata'\n",
    "    quiz = 324\n",
    "    \n",
    "elif no == 2:\n",
    "    target_species = 'Coccinella transversoguttata'\n",
    "    quiz = 510\n",
    "    \n",
    "elif no == 3:\n",
    "    target_species = 'Adalia bipunctata'\n",
    "    quiz = 1438\n",
    "    \n",
    "\n",
    "#ratio = int(input('삭제할 비율 입력 '))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad03556",
   "metadata": {},
   "source": [
    "# 머신러닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40386f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "c92_copy2[quiz:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040b0057",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# non SMOTE 버전\n",
    "\n",
    "print()\n",
    "print()\n",
    "print()\n",
    "print('<<<<주요종만sopr>>>>')\n",
    "\n",
    "# random state를 500번 셔플한 결과들의 평균\n",
    "\n",
    "hot = list(range (0, 100))\n",
    "pot = list(range(0, 100))\n",
    "\n",
    "df_score_all = pd.DataFrame({\"accuracy\":[],\"precision\":[], \"recall\":[], \"f1\":[], \"auc\":[], \"brier\":[], \"kappa\":[]})\n",
    "df_score_block = pd.DataFrame({\"accuracy\":[],\"precision\":[], \"recall\":[], \"f1\":[], \"auc\":[], \"brier\":[], \"kappa\":[]})\n",
    "\n",
    "T1 = 0\n",
    "T2 = 0\n",
    "T3 = 0\n",
    "T4 = 0\n",
    "T5 = 0\n",
    "T6 = 0\n",
    "T7 = 0\n",
    "\n",
    "accuracys = 0\n",
    "precisions = 0\n",
    "recalls = 0\n",
    "f1s = 0\n",
    "roc_aucs = 0\n",
    "brier_score = 0\n",
    "kappa_score = 0\n",
    "\n",
    "accuracy_all=[]\n",
    "precision_all=[]\n",
    "recall_all=[]\n",
    "f1score_all=[]\n",
    "auc_all=[]\n",
    "kappa_all=[]\n",
    "brier_all=[]\n",
    "\n",
    "accuracy_block=[]\n",
    "precision_block=[]\n",
    "recall_block=[]\n",
    "f1score_block=[]\n",
    "auc_block=[]\n",
    "kappa_block=[]\n",
    "brier_block=[]\n",
    "\n",
    "\n",
    "hist_Ep = []\n",
    "potcast = [] ####@@@@@@@###\n",
    "           \n",
    "for k in tqdm(hot):\n",
    "#     sample = c92_copy2[:1915].sample(n=288, random_state = k) # 07,08 제외시\n",
    "#     c92_copy22 = pd.concat([c92_copy2[1915:], sample])\n",
    "    BOX = c92_copy2[c92_copy2.Species_18 != 'species name'].reset_index(drop = True)\n",
    "    sample = BOX[quiz:].sample(n=quiz, random_state = k)\n",
    "    c92_copy22 = pd.concat([BOX[:quiz], sample])\n",
    "    \n",
    "    c92sr2_y = c92_copy22.target\n",
    "    \n",
    "    # MCz = c92_copy22[PTYPE]\n",
    "    MCz = c92_copy22[DJ]\n",
    "\n",
    "    for i in pot:\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(MCz, c92sr2_y, test_size=0.1, random_state= i) # 정확도는 0.1일 때 가장 높음\n",
    "\n",
    "        xgb_wrapper = XGBClassifier(n_estimators=1000, learning_rate=0.7, max_depth=7, objective = \"binary:logistic\")\n",
    "        evals = [(X_test, y_test)]\n",
    "        xgb_wrapper.fit(X_train, y_train, early_stopping_rounds=10, \n",
    "                            eval_metric=\"error\", eval_set=evals, verbose=0)\n",
    "        ws100_preds = xgb_wrapper.predict(X_test)\n",
    "        ws100_pred_proba = xgb_wrapper.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        # 예측 성능 평가\n",
    "        a, b, c, d, e = get_clf_eval_num(y_test, ws100_preds, ws100_pred_proba)\n",
    "        accuracys = accuracys + a\n",
    "        precisions = precisions + b\n",
    "        recalls = recalls + c\n",
    "        f1s = f1s + d\n",
    "        roc_aucs = roc_aucs + e\n",
    "        \n",
    "        f = brier_score_loss(y_test, ws100_pred_proba)\n",
    "        g = cohen_kappa_score(ws100_preds, y_test)\n",
    "        brier_score = brier_score + f\n",
    "        kappa_score = kappa_score + g\n",
    "        \n",
    "        accuracy_all.append(a)\n",
    "        precision_all.append(b)\n",
    "        recall_all.append(c)\n",
    "        f1score_all.append(d)\n",
    "        auc_all.append(e)\n",
    "        brier_all.append(f)\n",
    "        kappa_all.append(g)\n",
    "        \n",
    "        get_clf_eval(y_test, ws100_preds, ws100_pred_proba)\n",
    "        \n",
    "        hist_Ep.append(a)\n",
    "\n",
    "        mask = np.logical_not(np.equal(y_test, ws100_preds))\n",
    "#        print(f\"Elements wrong classified: {X_test[mask].index}\")\n",
    "        potcast.extend(mask.index.tolist())\n",
    "#         print(f\"Prediction by the model for each of those elements: {predictions[mask]}\")\n",
    "#         print(f\"Actual value for each of those elements: {np.asarray(y_test)[mask]}\")\n",
    "\n",
    "\n",
    "    print()\n",
    "    print()\n",
    "    print()\n",
    "#    print('<<<<주요종만sopr>>>>')\n",
    "\n",
    "    accuracys/len(pot)\n",
    "    precisions/len(pot)\n",
    "    recalls/len(pot)\n",
    "    f1s/len(pot)\n",
    "    roc_aucs/len(pot)\n",
    "    brier_score/len(pot)\n",
    "    kappa_score/len(pot)\n",
    "    \n",
    "               \n",
    "    T1 = T1 + (accuracys/len(pot))\n",
    "    T2 = T2 + (precisions/len(pot))\n",
    "    T3 = T3 + (recalls/len(pot))\n",
    "    T4 = T4 + (f1s/len(pot))\n",
    "    T5 = T5 + (roc_aucs/len(pot))\n",
    "    T6 = T6 + (brier_score/len(pot))\n",
    "    T7 = T7 + (kappa_score/len(pot))\n",
    "    \n",
    "    accuracy_block.append(accuracys/len(pot))\n",
    "    precision_block.append(precisions/len(pot))\n",
    "    recall_block.append(recalls/len(pot))\n",
    "    f1score_block.append(f1s/len(pot))\n",
    "    auc_block.append(roc_aucs/len(pot))\n",
    "    kappa_block.append(kappa_score/len(pot))\n",
    "    brier_block.append(brier_score/len(pot))\n",
    "        \n",
    "    accuracys = 0\n",
    "    precisions = 0\n",
    "    recalls = 0\n",
    "    f1s = 0\n",
    "    roc_aucs = 0\n",
    "    brier_score = 0\n",
    "    kappa_score = 0\n",
    "           \n",
    "T1/len(hot)\n",
    "T2/len(hot)\n",
    "T3/len(hot)\n",
    "T4/len(hot)\n",
    "T5/len(hot)\n",
    "T6/len(hot)\n",
    "T7/len(hot)\n",
    "\n",
    "\n",
    "num_bins = 20 # <- number of bins for the histogram\n",
    "plt.hist(hist_Ep, num_bins)\n",
    "plt.show()\n",
    "\n",
    "sum(hist_Ep)/len(hist_Ep)\n",
    "\n",
    "df_score_all['accuracy']=accuracy_all\n",
    "df_score_all['precision']=precision_all\n",
    "df_score_all['recall']=recall_all\n",
    "df_score_all['f1']=f1score_all\n",
    "df_score_all['auc']=auc_all\n",
    "df_score_all['brier']=brier_all\n",
    "df_score_all['kappa']=kappa_all\n",
    "\n",
    "df_score_block['accuracy']=accuracy_block\n",
    "df_score_block['precision']=precision_block\n",
    "df_score_block['recall']=recall_block\n",
    "df_score_block['f1']=f1score_block\n",
    "df_score_block['auc']=auc_block\n",
    "df_score_block['brier']=brier_block\n",
    "df_score_block['kappa']=kappa_block\n",
    "\n",
    "\n",
    "df_score_all.to_csv(\"./score_bag(output)/\"+str(target_species)+'_'+'DJ'+\"_scores_all.csv\")\n",
    "df_score_block.to_csv(\"./score_bag(output)/\"+str(target_species)+'_'+'DJ'+\"_scores_block.csv\")\n",
    "\n",
    "df_score_all.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5793bc5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# non SMOTE 버전\n",
    "\n",
    "print()\n",
    "print()\n",
    "print()\n",
    "print('<<<<주요종만sopr>>>>')\n",
    "\n",
    "# random state를 500번 셔플한 결과들의 평균\n",
    "\n",
    "hot = list(range (0, 100))\n",
    "pot = list(range(0, 100))\n",
    "\n",
    "df_score_all = pd.DataFrame({\"accuracy\":[],\"precision\":[], \"recall\":[], \"f1\":[], \"auc\":[], \"brier\":[], \"kappa\":[]})\n",
    "df_score_block = pd.DataFrame({\"accuracy\":[],\"precision\":[], \"recall\":[], \"f1\":[], \"auc\":[], \"brier\":[], \"kappa\":[]})\n",
    "\n",
    "T1 = 0\n",
    "T2 = 0\n",
    "T3 = 0\n",
    "T4 = 0\n",
    "T5 = 0\n",
    "T6 = 0\n",
    "T7 = 0\n",
    "\n",
    "accuracys = 0\n",
    "precisions = 0\n",
    "recalls = 0\n",
    "f1s = 0\n",
    "roc_aucs = 0\n",
    "brier_score = 0\n",
    "kappa_score = 0\n",
    "\n",
    "accuracy_all=[]\n",
    "precision_all=[]\n",
    "recall_all=[]\n",
    "f1score_all=[]\n",
    "auc_all=[]\n",
    "kappa_all=[]\n",
    "brier_all=[]\n",
    "\n",
    "accuracy_block=[]\n",
    "precision_block=[]\n",
    "recall_block=[]\n",
    "f1score_block=[]\n",
    "auc_block=[]\n",
    "kappa_block=[]\n",
    "brier_block=[]\n",
    "\n",
    "\n",
    "hist_Ep = []\n",
    "potcast = [] ####@@@@@@@###\n",
    "           \n",
    "for k in tqdm(hot):\n",
    "#     sample = c92_copy2[:1915].sample(n=288, random_state = k) # 07,08 제외시\n",
    "#     c92_copy22 = pd.concat([c92_copy2[1915:], sample])\n",
    "    BOX = c92_copy2[c92_copy2.Species_18 != 'species name'].reset_index(drop = True)\n",
    "    sample = BOX[quiz:].sample(n=quiz, random_state = k)\n",
    "    c92_copy22 = pd.concat([BOX[:quiz], sample])\n",
    "    \n",
    "    c92sr2_y = c92_copy22.target\n",
    "    \n",
    "    # MCz = c92_copy22[PTYPE]\n",
    "    MCz = c92_copy22[MC]\n",
    "\n",
    "    for i in pot:\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(MCz, c92sr2_y, test_size=0.1, random_state= i) # 정확도는 0.1일 때 가장 높음\n",
    "\n",
    "        xgb_wrapper = XGBClassifier(n_estimators=1000, learning_rate=0.7, max_depth=7, objective = \"binary:logistic\")\n",
    "        evals = [(X_test, y_test)]\n",
    "        xgb_wrapper.fit(X_train, y_train, early_stopping_rounds=10, \n",
    "                            eval_metric=\"error\", eval_set=evals, verbose=0)\n",
    "        ws100_preds = xgb_wrapper.predict(X_test)\n",
    "        ws100_pred_proba = xgb_wrapper.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        # 예측 성능 평가\n",
    "        a, b, c, d, e = get_clf_eval_num(y_test, ws100_preds, ws100_pred_proba)\n",
    "        accuracys = accuracys + a\n",
    "        precisions = precisions + b\n",
    "        recalls = recalls + c\n",
    "        f1s = f1s + d\n",
    "        roc_aucs = roc_aucs + e\n",
    "        \n",
    "        f = brier_score_loss(y_test, ws100_pred_proba)\n",
    "        g = cohen_kappa_score(ws100_preds, y_test)\n",
    "        brier_score = brier_score + f\n",
    "        kappa_score = kappa_score + g\n",
    "        \n",
    "        accuracy_all.append(a)\n",
    "        precision_all.append(b)\n",
    "        recall_all.append(c)\n",
    "        f1score_all.append(d)\n",
    "        auc_all.append(e)\n",
    "        brier_all.append(f)\n",
    "        kappa_all.append(g)\n",
    "        \n",
    "        get_clf_eval(y_test, ws100_preds, ws100_pred_proba)\n",
    "        \n",
    "        hist_Ep.append(a)\n",
    "\n",
    "        mask = np.logical_not(np.equal(y_test, ws100_preds))\n",
    "#        print(f\"Elements wrong classified: {X_test[mask].index}\")\n",
    "        potcast.extend(mask.index.tolist())\n",
    "#         print(f\"Prediction by the model for each of those elements: {predictions[mask]}\")\n",
    "#         print(f\"Actual value for each of those elements: {np.asarray(y_test)[mask]}\")\n",
    "\n",
    "\n",
    "    print()\n",
    "    print()\n",
    "    print()\n",
    "#    print('<<<<주요종만sopr>>>>')\n",
    "\n",
    "    accuracys/len(pot)\n",
    "    precisions/len(pot)\n",
    "    recalls/len(pot)\n",
    "    f1s/len(pot)\n",
    "    roc_aucs/len(pot)\n",
    "    brier_score/len(pot)\n",
    "    kappa_score/len(pot)\n",
    "    \n",
    "               \n",
    "    T1 = T1 + (accuracys/len(pot))\n",
    "    T2 = T2 + (precisions/len(pot))\n",
    "    T3 = T3 + (recalls/len(pot))\n",
    "    T4 = T4 + (f1s/len(pot))\n",
    "    T5 = T5 + (roc_aucs/len(pot))\n",
    "    T6 = T6 + (brier_score/len(pot))\n",
    "    T7 = T7 + (kappa_score/len(pot))\n",
    "    \n",
    "    accuracy_block.append(accuracys/len(pot))\n",
    "    precision_block.append(precisions/len(pot))\n",
    "    recall_block.append(recalls/len(pot))\n",
    "    f1score_block.append(f1s/len(pot))\n",
    "    auc_block.append(roc_aucs/len(pot))\n",
    "    kappa_block.append(kappa_score/len(pot))\n",
    "    brier_block.append(brier_score/len(pot))\n",
    "        \n",
    "    accuracys = 0\n",
    "    precisions = 0\n",
    "    recalls = 0\n",
    "    f1s = 0\n",
    "    roc_aucs = 0\n",
    "    brier_score = 0\n",
    "    kappa_score = 0\n",
    "           \n",
    "T1/len(hot)\n",
    "T2/len(hot)\n",
    "T3/len(hot)\n",
    "T4/len(hot)\n",
    "T5/len(hot)\n",
    "T6/len(hot)\n",
    "T7/len(hot)\n",
    "\n",
    "\n",
    "num_bins = 20 # <- number of bins for the histogram\n",
    "plt.hist(hist_Ep, num_bins)\n",
    "plt.show()\n",
    "\n",
    "sum(hist_Ep)/len(hist_Ep)\n",
    "\n",
    "df_score_all['accuracy']=accuracy_all\n",
    "df_score_all['precision']=precision_all\n",
    "df_score_all['recall']=recall_all\n",
    "df_score_all['f1']=f1score_all\n",
    "df_score_all['auc']=auc_all\n",
    "df_score_all['brier']=brier_all\n",
    "df_score_all['kappa']=kappa_all\n",
    "\n",
    "df_score_block['accuracy']=accuracy_block\n",
    "df_score_block['precision']=precision_block\n",
    "df_score_block['recall']=recall_block\n",
    "df_score_block['f1']=f1score_block\n",
    "df_score_block['auc']=auc_block\n",
    "df_score_block['brier']=brier_block\n",
    "df_score_block['kappa']=kappa_block\n",
    "\n",
    "\n",
    "df_score_all.to_csv(\"./score_bag(output)/\"+str(target_species)+'_'+'MC'+\"_scores_all.csv\")\n",
    "df_score_block.to_csv(\"./score_bag(output)/\"+str(target_species)+'_'+'MC'+\"_scores_block.csv\")\n",
    "\n",
    "df_score_all.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97eafdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# non SMOTE 버전\n",
    "\n",
    "print()\n",
    "print()\n",
    "print()\n",
    "print('<<<<주요종만sopr>>>>')\n",
    "\n",
    "# random state를 500번 셔플한 결과들의 평균\n",
    "\n",
    "hot = list(range (0, 100))\n",
    "pot = list(range(0, 100))\n",
    "\n",
    "df_score_all = pd.DataFrame({\"accuracy\":[],\"precision\":[], \"recall\":[], \"f1\":[], \"auc\":[], \"brier\":[], \"kappa\":[]})\n",
    "df_score_block = pd.DataFrame({\"accuracy\":[],\"precision\":[], \"recall\":[], \"f1\":[], \"auc\":[], \"brier\":[], \"kappa\":[]})\n",
    "\n",
    "T1 = 0\n",
    "T2 = 0\n",
    "T3 = 0\n",
    "T4 = 0\n",
    "T5 = 0\n",
    "T6 = 0\n",
    "T7 = 0\n",
    "\n",
    "accuracys = 0\n",
    "precisions = 0\n",
    "recalls = 0\n",
    "f1s = 0\n",
    "roc_aucs = 0\n",
    "brier_score = 0\n",
    "kappa_score = 0\n",
    "\n",
    "accuracy_all=[]\n",
    "precision_all=[]\n",
    "recall_all=[]\n",
    "f1score_all=[]\n",
    "auc_all=[]\n",
    "kappa_all=[]\n",
    "brier_all=[]\n",
    "\n",
    "accuracy_block=[]\n",
    "precision_block=[]\n",
    "recall_block=[]\n",
    "f1score_block=[]\n",
    "auc_block=[]\n",
    "kappa_block=[]\n",
    "brier_block=[]\n",
    "\n",
    "\n",
    "hist_Ep = []\n",
    "potcast = [] ####@@@@@@@###\n",
    "           \n",
    "for k in tqdm(hot):\n",
    "#     sample = c92_copy2[:1915].sample(n=288, random_state = k) # 07,08 제외시\n",
    "#     c92_copy22 = pd.concat([c92_copy2[1915:], sample])\n",
    "    BOX = c92_copy2[c92_copy2.Species_18 != 'species name'].reset_index(drop = True)\n",
    "    sample = BOX[quiz:].sample(n=quiz, random_state = k)\n",
    "    c92_copy22 = pd.concat([BOX[:quiz], sample])\n",
    "    \n",
    "    c92sr2_y = c92_copy22.target\n",
    "    \n",
    "    # MCz = c92_copy22[PTYPE]\n",
    "    MCz = c92_copy22[BEATMKR]\n",
    "\n",
    "    for i in pot:\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(MCz, c92sr2_y, test_size=0.1, random_state= i) # 정확도는 0.1일 때 가장 높음\n",
    "\n",
    "        xgb_wrapper = XGBClassifier(n_estimators=1000, learning_rate=0.7, max_depth=7, objective = \"binary:logistic\")\n",
    "        evals = [(X_test, y_test)]\n",
    "        xgb_wrapper.fit(X_train, y_train, early_stopping_rounds=10, \n",
    "                            eval_metric=\"error\", eval_set=evals, verbose=0)\n",
    "        ws100_preds = xgb_wrapper.predict(X_test)\n",
    "        ws100_pred_proba = xgb_wrapper.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        # 예측 성능 평가\n",
    "        a, b, c, d, e = get_clf_eval_num(y_test, ws100_preds, ws100_pred_proba)\n",
    "        accuracys = accuracys + a\n",
    "        precisions = precisions + b\n",
    "        recalls = recalls + c\n",
    "        f1s = f1s + d\n",
    "        roc_aucs = roc_aucs + e\n",
    "        \n",
    "        f = brier_score_loss(y_test, ws100_pred_proba)\n",
    "        g = cohen_kappa_score(ws100_preds, y_test)\n",
    "        brier_score = brier_score + f\n",
    "        kappa_score = kappa_score + g\n",
    "        \n",
    "        accuracy_all.append(a)\n",
    "        precision_all.append(b)\n",
    "        recall_all.append(c)\n",
    "        f1score_all.append(d)\n",
    "        auc_all.append(e)\n",
    "        brier_all.append(f)\n",
    "        kappa_all.append(g)\n",
    "        \n",
    "        get_clf_eval(y_test, ws100_preds, ws100_pred_proba)\n",
    "        \n",
    "        hist_Ep.append(a)\n",
    "\n",
    "        mask = np.logical_not(np.equal(y_test, ws100_preds))\n",
    "#        print(f\"Elements wrong classified: {X_test[mask].index}\")\n",
    "        potcast.extend(mask.index.tolist())\n",
    "#         print(f\"Prediction by the model for each of those elements: {predictions[mask]}\")\n",
    "#         print(f\"Actual value for each of those elements: {np.asarray(y_test)[mask]}\")\n",
    "\n",
    "\n",
    "    print()\n",
    "    print()\n",
    "    print()\n",
    "#    print('<<<<주요종만sopr>>>>')\n",
    "\n",
    "    accuracys/len(pot)\n",
    "    precisions/len(pot)\n",
    "    recalls/len(pot)\n",
    "    f1s/len(pot)\n",
    "    roc_aucs/len(pot)\n",
    "    brier_score/len(pot)\n",
    "    kappa_score/len(pot)\n",
    "    \n",
    "               \n",
    "    T1 = T1 + (accuracys/len(pot))\n",
    "    T2 = T2 + (precisions/len(pot))\n",
    "    T3 = T3 + (recalls/len(pot))\n",
    "    T4 = T4 + (f1s/len(pot))\n",
    "    T5 = T5 + (roc_aucs/len(pot))\n",
    "    T6 = T6 + (brier_score/len(pot))\n",
    "    T7 = T7 + (kappa_score/len(pot))\n",
    "    \n",
    "    accuracy_block.append(accuracys/len(pot))\n",
    "    precision_block.append(precisions/len(pot))\n",
    "    recall_block.append(recalls/len(pot))\n",
    "    f1score_block.append(f1s/len(pot))\n",
    "    auc_block.append(roc_aucs/len(pot))\n",
    "    kappa_block.append(kappa_score/len(pot))\n",
    "    brier_block.append(brier_score/len(pot))\n",
    "        \n",
    "    accuracys = 0\n",
    "    precisions = 0\n",
    "    recalls = 0\n",
    "    f1s = 0\n",
    "    roc_aucs = 0\n",
    "    brier_score = 0\n",
    "    kappa_score = 0\n",
    "           \n",
    "T1/len(hot)\n",
    "T2/len(hot)\n",
    "T3/len(hot)\n",
    "T4/len(hot)\n",
    "T5/len(hot)\n",
    "T6/len(hot)\n",
    "T7/len(hot)\n",
    "\n",
    "\n",
    "num_bins = 20 # <- number of bins for the histogram\n",
    "plt.hist(hist_Ep, num_bins)\n",
    "plt.show()\n",
    "\n",
    "sum(hist_Ep)/len(hist_Ep)\n",
    "\n",
    "df_score_all['accuracy']=accuracy_all\n",
    "df_score_all['precision']=precision_all\n",
    "df_score_all['recall']=recall_all\n",
    "df_score_all['f1']=f1score_all\n",
    "df_score_all['auc']=auc_all\n",
    "df_score_all['brier']=brier_all\n",
    "df_score_all['kappa']=kappa_all\n",
    "\n",
    "df_score_block['accuracy']=accuracy_block\n",
    "df_score_block['precision']=precision_block\n",
    "df_score_block['recall']=recall_block\n",
    "df_score_block['f1']=f1score_block\n",
    "df_score_block['auc']=auc_block\n",
    "df_score_block['brier']=brier_block\n",
    "df_score_block['kappa']=kappa_block\n",
    "\n",
    "\n",
    "df_score_all.to_csv(\"./score_bag(output)/\"+str(target_species)+'_'+'BTMKR'+\"_scores_all.csv\")\n",
    "df_score_block.to_csv(\"./score_bag(output)/\"+str(target_species)+'_'+'BTMKR'+\"_scores_block.csv\")\n",
    "\n",
    "df_score_all.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2994985c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# non SMOTE 버전\n",
    "\n",
    "print()\n",
    "print()\n",
    "print()\n",
    "print('<<<<주요종만sopr>>>>')\n",
    "\n",
    "# random state를 500번 셔플한 결과들의 평균\n",
    "\n",
    "hot = list(range (0, 100))\n",
    "pot = list(range(0, 100))\n",
    "\n",
    "df_score_all = pd.DataFrame({\"accuracy\":[],\"precision\":[], \"recall\":[], \"f1\":[], \"auc\":[], \"brier\":[], \"kappa\":[]})\n",
    "df_score_block = pd.DataFrame({\"accuracy\":[],\"precision\":[], \"recall\":[], \"f1\":[], \"auc\":[], \"brier\":[], \"kappa\":[]})\n",
    "\n",
    "T1 = 0\n",
    "T2 = 0\n",
    "T3 = 0\n",
    "T4 = 0\n",
    "T5 = 0\n",
    "T6 = 0\n",
    "T7 = 0\n",
    "\n",
    "accuracys = 0\n",
    "precisions = 0\n",
    "recalls = 0\n",
    "f1s = 0\n",
    "roc_aucs = 0\n",
    "brier_score = 0\n",
    "kappa_score = 0\n",
    "\n",
    "accuracy_all=[]\n",
    "precision_all=[]\n",
    "recall_all=[]\n",
    "f1score_all=[]\n",
    "auc_all=[]\n",
    "kappa_all=[]\n",
    "brier_all=[]\n",
    "\n",
    "accuracy_block=[]\n",
    "precision_block=[]\n",
    "recall_block=[]\n",
    "f1score_block=[]\n",
    "auc_block=[]\n",
    "kappa_block=[]\n",
    "brier_block=[]\n",
    "\n",
    "\n",
    "hist_Ep = []\n",
    "potcast = [] ####@@@@@@@###\n",
    "           \n",
    "for k in tqdm(hot):\n",
    "#     sample = c92_copy2[:1915].sample(n=288, random_state = k) # 07,08 제외시\n",
    "#     c92_copy22 = pd.concat([c92_copy2[1915:], sample])\n",
    "    BOX = c92_copy2[c92_copy2.Species_18 != 'species name'].reset_index(drop = True)\n",
    "    sample = BOX[quiz:].sample(n=quiz, random_state = k)\n",
    "    c92_copy22 = pd.concat([BOX[:quiz], sample])\n",
    "    \n",
    "    c92sr2_y = c92_copy22.target\n",
    "    \n",
    "    MCz = c92_copy22[PTYPE]\n",
    "#    MCz = c92_copy22[DJ]\n",
    "\n",
    "    for i in pot:\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(MCz, c92sr2_y, test_size=0.1, random_state= i) # 정확도는 0.1일 때 가장 높음\n",
    "\n",
    "        xgb_wrapper = XGBClassifier(n_estimators=1000, learning_rate=0.7, max_depth=7, objective = \"binary:logistic\")\n",
    "        evals = [(X_test, y_test)]\n",
    "        xgb_wrapper.fit(X_train, y_train, early_stopping_rounds=10, \n",
    "                            eval_metric=\"error\", eval_set=evals, verbose=0)\n",
    "        ws100_preds = xgb_wrapper.predict(X_test)\n",
    "        ws100_pred_proba = xgb_wrapper.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        # 예측 성능 평가\n",
    "        a, b, c, d, e = get_clf_eval_num(y_test, ws100_preds, ws100_pred_proba)\n",
    "        accuracys = accuracys + a\n",
    "        precisions = precisions + b\n",
    "        recalls = recalls + c\n",
    "        f1s = f1s + d\n",
    "        roc_aucs = roc_aucs + e\n",
    "        \n",
    "        f = brier_score_loss(y_test, ws100_pred_proba)\n",
    "        g = cohen_kappa_score(ws100_preds, y_test)\n",
    "        brier_score = brier_score + f\n",
    "        kappa_score = kappa_score + g\n",
    "        \n",
    "        accuracy_all.append(a)\n",
    "        precision_all.append(b)\n",
    "        recall_all.append(c)\n",
    "        f1score_all.append(d)\n",
    "        auc_all.append(e)\n",
    "        brier_all.append(f)\n",
    "        kappa_all.append(g)\n",
    "        \n",
    "        get_clf_eval(y_test, ws100_preds, ws100_pred_proba)\n",
    "        \n",
    "        hist_Ep.append(a)\n",
    "\n",
    "        mask = np.logical_not(np.equal(y_test, ws100_preds))\n",
    "#        print(f\"Elements wrong classified: {X_test[mask].index}\")\n",
    "        potcast.extend(mask.index.tolist())\n",
    "#         print(f\"Prediction by the model for each of those elements: {predictions[mask]}\")\n",
    "#         print(f\"Actual value for each of those elements: {np.asarray(y_test)[mask]}\")\n",
    "\n",
    "\n",
    "    print()\n",
    "    print()\n",
    "    print()\n",
    "#    print('<<<<주요종만sopr>>>>')\n",
    "\n",
    "    accuracys/len(pot)\n",
    "    precisions/len(pot)\n",
    "    recalls/len(pot)\n",
    "    f1s/len(pot)\n",
    "    roc_aucs/len(pot)\n",
    "    brier_score/len(pot)\n",
    "    kappa_score/len(pot)\n",
    "    \n",
    "               \n",
    "    T1 = T1 + (accuracys/len(pot))\n",
    "    T2 = T2 + (precisions/len(pot))\n",
    "    T3 = T3 + (recalls/len(pot))\n",
    "    T4 = T4 + (f1s/len(pot))\n",
    "    T5 = T5 + (roc_aucs/len(pot))\n",
    "    T6 = T6 + (brier_score/len(pot))\n",
    "    T7 = T7 + (kappa_score/len(pot))\n",
    "    \n",
    "    accuracy_block.append(accuracys/len(pot))\n",
    "    precision_block.append(precisions/len(pot))\n",
    "    recall_block.append(recalls/len(pot))\n",
    "    f1score_block.append(f1s/len(pot))\n",
    "    auc_block.append(roc_aucs/len(pot))\n",
    "    kappa_block.append(kappa_score/len(pot))\n",
    "    brier_block.append(brier_score/len(pot))\n",
    "        \n",
    "    accuracys = 0\n",
    "    precisions = 0\n",
    "    recalls = 0\n",
    "    f1s = 0\n",
    "    roc_aucs = 0\n",
    "    brier_score = 0\n",
    "    kappa_score = 0\n",
    "           \n",
    "T1/len(hot)\n",
    "T2/len(hot)\n",
    "T3/len(hot)\n",
    "T4/len(hot)\n",
    "T5/len(hot)\n",
    "T6/len(hot)\n",
    "T7/len(hot)\n",
    "\n",
    "\n",
    "num_bins = 20 # <- number of bins for the histogram\n",
    "plt.hist(hist_Ep, num_bins)\n",
    "plt.show()\n",
    "\n",
    "sum(hist_Ep)/len(hist_Ep)\n",
    "\n",
    "df_score_all['accuracy']=accuracy_all\n",
    "df_score_all['precision']=precision_all\n",
    "df_score_all['recall']=recall_all\n",
    "df_score_all['f1']=f1score_all\n",
    "df_score_all['auc']=auc_all\n",
    "df_score_all['brier']=brier_all\n",
    "df_score_all['kappa']=kappa_all\n",
    "\n",
    "df_score_block['accuracy']=accuracy_block\n",
    "df_score_block['precision']=precision_block\n",
    "df_score_block['recall']=recall_block\n",
    "df_score_block['f1']=f1score_block\n",
    "df_score_block['auc']=auc_block\n",
    "df_score_block['brier']=brier_block\n",
    "df_score_block['kappa']=kappa_block\n",
    "\n",
    "\n",
    "df_score_all.to_csv(\"./score_bag(output)/\"+str(target_species)+'_'+'PTYPE'+\"_scores_all.csv\")\n",
    "df_score_block.to_csv(\"./score_bag(output)/\"+str(target_species)+'_'+'PTYPE'+\"_scores_block.csv\")\n",
    "\n",
    "df_score_all.describe()\n",
    "\n",
    "\n",
    "################# 탈 부착부 error frequency\n",
    "################# 탈 부착부 error frequency\n",
    "################# 탈 부착부 error frequency\n",
    "################# 탈 부착부 error frequency\n",
    "b = Counter(potcast)\n",
    "\n",
    "taggerz = pd.DataFrame.from_dict(b, orient ='index')\n",
    "taggerz.rename(columns={0:'error_frequency'}, inplace=True)\n",
    "\n",
    "traffic_jam = pd.merge(c92zet, taggerz, left_index=True, right_index=True, how='left')\n",
    "traffic_jam = traffic_jam.fillna(0)\n",
    "\n",
    "#traffic_jam['error_frequency']\n",
    "\n",
    "traffic_jam['error_chance'] = 0\n",
    "traffic_jam['error_chance'][:quiz] = traffic_jam['error_frequency'][:quiz]/(len(hot)*len(pot))\n",
    "traffic_jam['error_chance'][quiz:] = traffic_jam['error_frequency'][quiz:]/((len(c92_copy2[:quiz].index)*(len(hot)*len(pot)))/len(c92_copy2[quiz:].index))\n",
    "#traffic_jam['error_chance']\n",
    "# traffic_jam.sort_values(by='error_chance', axis=0, ascending=False, inplace=False)\n",
    "\n",
    "traffic_jam2 = traffic_jam[traffic_jam.target==1].sort_values(by='Year', axis=0, ascending=False, inplace=False)\n",
    "\n",
    "traffic_jam3 = traffic_jam[traffic_jam.target==0].sort_values(by='Year', axis=0, ascending=False, inplace=False)\n",
    "\n",
    "\n",
    "traffic_jam2_5 = traffic_jam2.groupby('Year').error_frequency.mean()\n",
    "\n",
    "plt.bar(traffic_jam2_5.index, traffic_jam2_5)\n",
    "\n",
    "plt.title('Presence', fontsize=20)\n",
    "\n",
    "plt.xlabel('Year', fontsize=18)\n",
    "\n",
    "plt.ylabel('error_chance', fontsize=18)\n",
    "\n",
    "plt.savefig('./boxplot(output)/'+str(target_species)+'_'+'PTYPE'+\"_(presence_error_chance).pdf\", dpi=300)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "traffic_jam3_5 = traffic_jam3.groupby('Year').error_frequency.mean()\n",
    "\n",
    "plt.bar(traffic_jam3_5.index, traffic_jam3_5)\n",
    "\n",
    "plt.title('Absence', fontsize=20)\n",
    "\n",
    "plt.xlabel('Year', fontsize=18)\n",
    "\n",
    "plt.ylabel('error_chance', fontsize=18)\n",
    "\n",
    "plt.savefig('./boxplot(output)/'+str(target_species)+'_'+'PTYPE'+\"_(absence_error_chance).pdf\", dpi=300)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b83d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_score_all.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611149fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##### split 비율 변경\n",
    "\n",
    "#non SMOTE 버전\n",
    "\n",
    "print()\n",
    "print()\n",
    "print()\n",
    "print('<<<<주요종만sopr>>>>')\n",
    "\n",
    "# random state를 500번 셔플한 결과들의 평균\n",
    "\n",
    "hot = list(range (0, 50))\n",
    "pot = list(range(0, 50))\n",
    "\n",
    "df_score_all = pd.DataFrame({\"accuracy\":[],\"precision\":[], \"recall\":[], \"f1\":[], \"auc\":[], \"brier\":[], \"kappa\":[]})\n",
    "df_score_block = pd.DataFrame({\"accuracy\":[],\"precision\":[], \"recall\":[], \"f1\":[], \"auc\":[], \"brier\":[], \"kappa\":[]})\n",
    "\n",
    "T1 = 0\n",
    "T2 = 0\n",
    "T3 = 0\n",
    "T4 = 0\n",
    "T5 = 0\n",
    "T6 = 0\n",
    "T7 = 0\n",
    "\n",
    "accuracys = 0\n",
    "precisions = 0\n",
    "recalls = 0\n",
    "f1s = 0\n",
    "roc_aucs = 0\n",
    "brier_score = 0\n",
    "kappa_score = 0\n",
    "\n",
    "accuracy_all=[]\n",
    "precision_all=[]\n",
    "recall_all=[]\n",
    "f1score_all=[]\n",
    "auc_all=[]\n",
    "kappa_all=[]\n",
    "brier_all=[]\n",
    "\n",
    "accuracy_block=[]\n",
    "precision_block=[]\n",
    "recall_block=[]\n",
    "f1score_block=[]\n",
    "auc_block=[]\n",
    "kappa_block=[]\n",
    "brier_block=[]\n",
    "\n",
    "\n",
    "hist_Ep = []\n",
    "potcast = [] ####@@@@@@@###\n",
    "           \n",
    "for k in tqdm(hot):\n",
    "#     sample = c92_copy2[:1915].sample(n=288, random_state = k) # 07,08 제외시\n",
    "#     c92_copy22 = pd.concat([c92_copy2[1915:], sample])\n",
    "    BOX = c92_copy2[c92_copy2.Species_18 != 'species name'].reset_index(drop = True)\n",
    "    sample = BOX[quiz:].sample(n=quiz, random_state = k)\n",
    "    c92_copy22 = pd.concat([BOX[:quiz], sample])\n",
    "    \n",
    "    c92sr2_y = c92_copy22.target\n",
    "    \n",
    "    # MCz = c92_copy22[PTYPE]\n",
    "    MCz = c92_copy22[DJ]\n",
    "\n",
    "    for i in pot:\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(MCz, c92sr2_y, test_size=0.3, random_state= i) # 정확도는 0.1일 때 가장 높음\n",
    "\n",
    "        xgb_wrapper = XGBClassifier(n_estimators=1000, learning_rate=0.7, max_depth=7, objective = \"binary:logistic\")\n",
    "        evals = [(X_test, y_test)]\n",
    "        xgb_wrapper.fit(X_train, y_train, early_stopping_rounds=10, \n",
    "                            eval_metric=\"error\", eval_set=evals, verbose=0)\n",
    "        ws100_preds = xgb_wrapper.predict(X_test)\n",
    "        ws100_pred_proba = xgb_wrapper.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        # 예측 성능 평가\n",
    "        a, b, c, d, e = get_clf_eval_num(y_test, ws100_preds, ws100_pred_proba)\n",
    "        accuracys = accuracys + a\n",
    "        precisions = precisions + b\n",
    "        recalls = recalls + c\n",
    "        f1s = f1s + d\n",
    "        roc_aucs = roc_aucs + e\n",
    "        \n",
    "        f = brier_score_loss(y_test, ws100_pred_proba)\n",
    "        g = cohen_kappa_score(ws100_preds, y_test)\n",
    "        brier_score = brier_score + f\n",
    "        kappa_score = kappa_score + g\n",
    "        \n",
    "        accuracy_all.append(a)\n",
    "        precision_all.append(b)\n",
    "        recall_all.append(c)\n",
    "        f1score_all.append(d)\n",
    "        auc_all.append(e)\n",
    "        brier_all.append(f)\n",
    "        kappa_all.append(g)\n",
    "        \n",
    "        get_clf_eval(y_test, ws100_preds, ws100_pred_proba)\n",
    "        \n",
    "        hist_Ep.append(a)\n",
    "\n",
    "        mask = np.logical_not(np.equal(y_test, ws100_preds))\n",
    "#        print(f\"Elements wrong classified: {X_test[mask].index}\")\n",
    "        potcast.extend(mask.index.tolist())\n",
    "#         print(f\"Prediction by the model for each of those elements: {predictions[mask]}\")\n",
    "#         print(f\"Actual value for each of those elements: {np.asarray(y_test)[mask]}\")\n",
    "\n",
    "\n",
    "    print()\n",
    "    print()\n",
    "    print()\n",
    "#    print('<<<<주요종만sopr>>>>')\n",
    "\n",
    "    accuracys/len(pot)\n",
    "    precisions/len(pot)\n",
    "    recalls/len(pot)\n",
    "    f1s/len(pot)\n",
    "    roc_aucs/len(pot)\n",
    "    brier_score/len(pot)\n",
    "    kappa_score/len(pot)\n",
    "    \n",
    "               \n",
    "    T1 = T1 + (accuracys/len(pot))\n",
    "    T2 = T2 + (precisions/len(pot))\n",
    "    T3 = T3 + (recalls/len(pot))\n",
    "    T4 = T4 + (f1s/len(pot))\n",
    "    T5 = T5 + (roc_aucs/len(pot))\n",
    "    T6 = T6 + (brier_score/len(pot))\n",
    "    T7 = T7 + (kappa_score/len(pot))\n",
    "    \n",
    "    accuracy_block.append(accuracys/len(pot))\n",
    "    precision_block.append(precisions/len(pot))\n",
    "    recall_block.append(recalls/len(pot))\n",
    "    f1score_block.append(f1s/len(pot))\n",
    "    auc_block.append(roc_aucs/len(pot))\n",
    "    kappa_block.append(kappa_score/len(pot))\n",
    "    brier_block.append(brier_score/len(pot))\n",
    "        \n",
    "    accuracys = 0\n",
    "    precisions = 0\n",
    "    recalls = 0\n",
    "    f1s = 0\n",
    "    roc_aucs = 0\n",
    "    brier_score = 0\n",
    "    kappa_score = 0\n",
    "           \n",
    "T1/len(hot)\n",
    "T2/len(hot)\n",
    "T3/len(hot)\n",
    "T4/len(hot)\n",
    "T5/len(hot)\n",
    "T6/len(hot)\n",
    "T7/len(hot)\n",
    "\n",
    "\n",
    "num_bins = 20 # <- number of bins for the histogram\n",
    "plt.hist(hist_Ep, num_bins)\n",
    "plt.show()\n",
    "\n",
    "sum(hist_Ep)/len(hist_Ep)\n",
    "\n",
    "df_score_all['accuracy']=accuracy_all\n",
    "df_score_all['precision']=precision_all\n",
    "df_score_all['recall']=recall_all\n",
    "df_score_all['f1']=f1score_all\n",
    "df_score_all['auc']=auc_all\n",
    "df_score_all['brier']=brier_all\n",
    "df_score_all['kappa']=kappa_all\n",
    "\n",
    "df_score_block['accuracy']=accuracy_block\n",
    "df_score_block['precision']=precision_block\n",
    "df_score_block['recall']=recall_block\n",
    "df_score_block['f1']=f1score_block\n",
    "df_score_block['auc']=auc_block\n",
    "df_score_block['brier']=brier_block\n",
    "df_score_block['kappa']=kappa_block\n",
    "\n",
    "\n",
    "df_score_all.to_csv(\"./score_bag(output)/\"+str(target_species)+'_'+'DJ'+'30%'+\"_scores_all.csv\")\n",
    "df_score_block.to_csv(\"./score_bag(output)/\"+str(target_species)+'_'+'DJ'+'30%'+\"_scores_block.csv\")\n",
    "\n",
    "df_score_all.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850692ed",
   "metadata": {},
   "source": [
    "# SHAP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff00d5f",
   "metadata": {},
   "source": [
    "## feature importance selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d2b915",
   "metadata": {},
   "outputs": [],
   "source": [
    "PTYPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d744cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# non SMOTE 버전\n",
    "\n",
    "print()\n",
    "print()\n",
    "print()\n",
    "print('<<<<주요종만sopr>>>>')\n",
    "\n",
    "# random state를 500번 셔플한 결과들의 평균\n",
    "\n",
    "# hot = list(range (0, 25))\n",
    "# pot = list(range(0, 25)) \n",
    "\n",
    "hot = list(range (0, 25))\n",
    "pot = list(range(0, 25)) \n",
    "           \n",
    "T1 = 0\n",
    "T2 = 0\n",
    "T3 = 0\n",
    "T4 = 0\n",
    "T5 = 0\n",
    "\n",
    "accuracys = 0\n",
    "precisions = 0\n",
    "recalls = 0\n",
    "f1s = 0\n",
    "roc_aucs = 0\n",
    "\n",
    "potcast = [] ####@@@@@@@###\n",
    "\n",
    "captain_Q_all = pd.DataFrame(columns=['name','shap_index'])\n",
    "captain_Q_75 = pd.DataFrame(columns=['name','shap_index'])\n",
    "captain_Q_95 = pd.DataFrame(columns=['name','shap_index'])\n",
    "\n",
    "for k in tqdm(hot):\n",
    "#     sample = c92_copy2[:1915].sample(n=288, random_state = k) # 07,08 제외시\n",
    "#     c92_copy22 = pd.concat([c92_copy2[1915:], sample])\n",
    "#     sample = c92_copy2[294:].sample(n=294, random_state = k)\n",
    "#     c92_copy22 = pd.concat([c92_copy2[:294], sample])\n",
    "\n",
    "    BOX = c92_copy2[c92_copy2.Species_18 != 'a species'].reset_index(drop = True)\n",
    "    sample = BOX[quiz:].sample(n=quiz, random_state = k)\n",
    "    c92_copy22 = pd.concat([BOX[:quiz], sample])\n",
    "    \n",
    "    c92sr2_y = c92_copy22.target\n",
    "    \n",
    "    #################\n",
    "    LABEL = PTYPE ##### 여기에 변수이름 입력하기!!!\n",
    "    MCz = c92_copy22[LABEL]\n",
    "\n",
    "    for i in pot:\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(MCz, c92sr2_y, test_size=0.1, random_state= i) # 정확도는 0.1일 때 가장 높음\n",
    "\n",
    "        xgb_wrapper = XGBClassifier(n_estimators=1000, learning_rate=0.7, max_depth=7, objective = \"binary:logistic\")\n",
    "        evals = [(X_test, y_test)]\n",
    "        xgb_wrapper.fit(X_train, y_train, early_stopping_rounds=10, \n",
    "                            eval_metric=\"error\", eval_set=evals, verbose=0)\n",
    "        ws100_preds = xgb_wrapper.predict(X_test)\n",
    "        ws100_pred_proba = xgb_wrapper.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "\n",
    "        # 예측 성능 평가\n",
    "        a, b, c, d, e = get_clf_eval_num(y_test, ws100_preds, ws100_pred_proba)\n",
    "        accuracys = accuracys + a\n",
    "        precisions = precisions + b\n",
    "        recalls = recalls + c\n",
    "        f1s = f1s + d\n",
    "        roc_aucs = roc_aucs + e\n",
    "\n",
    "        get_clf_eval(y_test, ws100_preds, ws100_pred_proba)\n",
    "        \n",
    "        \n",
    "        shap.initjs()\n",
    "        explainer = shap.TreeExplainer(xgb_wrapper)\n",
    "        shap_values = explainer.shap_values(X_train)\n",
    "\n",
    "        vals= np.abs(shap_values).mean(0)\n",
    "        feature_importance = pd.DataFrame(list(zip(X_train.columns,vals)),columns=['col_name','feature_importance_vals'])\n",
    "        feature_importance.sort_values(by=['feature_importance_vals'],ascending=False,inplace=True)\n",
    "        \n",
    "        captain_Q_all = pd.concat([captain_Q_all, feature_importance])\n",
    "        \n",
    "        \n",
    "#         if accuracys > 0.95:\n",
    "#             print('accuracys > 0.95')\n",
    "            \n",
    "# #            shap.summary_plot(shap_values, X_train)\n",
    "#             explainer = shap.TreeExplainer(xgb_wrapper)\n",
    "#             shap_values = explainer.shap_values(X_train)\n",
    "        \n",
    "#             vals= np.abs(shap_values).mean(0)\n",
    "#             feature_importance = pd.DataFrame(list(zip(X_train.columns,vals)),columns=['col_name','feature_importance_vals'])\n",
    "#             feature_importance.sort_values(by=['feature_importance_vals'],ascending=False,inplace=True)\n",
    "            \n",
    "#             captain_Q_95 = pd.concat([captain_Q_95, feature_importance])\n",
    "            \n",
    "            \n",
    "#             b = Counter(potcast)\n",
    "\n",
    "# #             taggerz = pd.DataFrame.from_dict(b, orient ='index')\n",
    "# #             taggerz.rename(columns={0:'error_frequency'}, inplace=True)\n",
    "\n",
    "# #             traffic_jam = pd.merge(c92zet, taggerz, left_index=True, right_index=True, how='left')\n",
    "# #             traffic_jam = traffic_jam.fillna(0)\n",
    "\n",
    "# # #            traffic_jam['error_frequency']\n",
    "\n",
    "# #             traffic_jam['error_chance'] = 0\n",
    "# #             traffic_jam['error_chance'][:294] = traffic_jam['error_frequency'][:294]/(len(hot)*len(pot))\n",
    "# #             traffic_jam['error_chance'][294:] = traffic_jam['error_frequency'][294:]/((len(c92_copy2[:294].index)*(len(hot)*len(pot)))/len(c92_copy2[294:].index))\n",
    "# # #            traffic_jam['error_chance']\n",
    "\n",
    "# # #            traffic_jam.sort_values(by='error_chance', axis=0, ascending=False, inplace=False)\n",
    "\n",
    "# #             traffic_jam2 = traffic_jam[traffic_jam.target==1].sort_values(by='Year', axis=0, ascending=False, inplace=False)\n",
    "\n",
    "# #             traffic_jam3 = traffic_jam[traffic_jam.target==0].sort_values(by='Year', axis=0, ascending=False, inplace=False)\n",
    "\n",
    "\n",
    "# #             traffic_jam2_5 = traffic_jam2.groupby('Year').error_frequency.mean()\n",
    "\n",
    "# # #             plt.bar(traffic_jam2_5.index, traffic_jam2_5)\n",
    "\n",
    "# # #             plt.title('Presence', fontsize=20)\n",
    "\n",
    "# # #             plt.xlabel('Year', fontsize=18)\n",
    "\n",
    "# # #             plt.ylabel('error_chance', fontsize=18)\n",
    "\n",
    "# # #             plt.show()\n",
    "\n",
    "\n",
    "# # #             traffic_jam3_5 = traffic_jam3.groupby('Year').error_frequency.mean()\n",
    "\n",
    "# # #             plt.bar(traffic_jam3_5.index, traffic_jam3_5)\n",
    "\n",
    "# # #             plt.title('Absence', fontsize=20)\n",
    "\n",
    "# # #             plt.xlabel('Year', fontsize=18)\n",
    "\n",
    "# # #             plt.ylabel('error_chance', fontsize=18)\n",
    "\n",
    "# # #             plt.show()\n",
    "            \n",
    "            \n",
    "# #             dice = X_train.index.tolist()\n",
    "# #             NAMS = traffic_jam.iloc[dice]\n",
    "# # #             NAMS['Species_18'].value_counts(normalize=True)\n",
    "# # #             NAMS['Year'].value_counts()\n",
    "\n",
    "            \n",
    "            \n",
    "#         elif accuracys < 0.75:\n",
    "#             print('accuracys < 0.75')\n",
    "            \n",
    "#             explainer = shap.TreeExplainer(xgb_wrapper)\n",
    "#             shap_values = explainer.shap_values(X_train)\n",
    "            \n",
    "\n",
    "            \n",
    "#             vals= np.abs(shap_values).mean(0)\n",
    "#             feature_importance = pd.DataFrame(list(zip(X_train.columns,vals)),columns=['col_name','feature_importance_vals'])\n",
    "#             feature_importance.sort_values(by=['feature_importance_vals'],ascending=False,inplace=True)\n",
    "#             captain_Q_75 = pd.concat([captain_Q_75, feature_importance])\n",
    "  \n",
    "            \n",
    "#             b = Counter(potcast)\n",
    "\n",
    "# #             taggerz = pd.DataFrame.from_dict(b, orient ='index')\n",
    "# #             taggerz.rename(columns={0:'error_frequency'}, inplace=True)\n",
    "\n",
    "# #             traffic_jam = pd.merge(c92zet, taggerz, left_index=True, right_index=True, how='left')\n",
    "# #             traffic_jam = traffic_jam.fillna(0)\n",
    "\n",
    "# # #            traffic_jam['error_frequency']\n",
    "\n",
    "# #             traffic_jam['error_chance'] = 0\n",
    "# #             traffic_jam['error_chance'][:294] = traffic_jam['error_frequency'][:294]/(len(hot)*len(pot))\n",
    "# #             traffic_jam['error_chance'][294:] = traffic_jam['error_frequency'][294:]/((len(c92_copy2[:294].index)*(len(hot)*len(pot)))/len(c92_copy2[294:].index))\n",
    "# # #            traffic_jam['error_chance']\n",
    "\n",
    "# # #            traffic_jam.sort_values(by='error_chance', axis=0, ascending=False, inplace=False)\n",
    "\n",
    "# #             traffic_jam2 = traffic_jam[traffic_jam.target==1].sort_values(by='Year', axis=0, ascending=False, inplace=False)\n",
    "\n",
    "# #             traffic_jam3 = traffic_jam[traffic_jam.target==0].sort_values(by='Year', axis=0, ascending=False, inplace=False)\n",
    "\n",
    "\n",
    "# #             traffic_jam2_5 = traffic_jam2.groupby('Year').error_frequency.mean()\n",
    "\n",
    "# # #             plt.bar(traffic_jam2_5.index, traffic_jam2_5)\n",
    "\n",
    "# # #             plt.title('Presence', fontsize=20)\n",
    "\n",
    "# # #             plt.xlabel('Year', fontsize=18)\n",
    "\n",
    "# # #             plt.ylabel('error_chance', fontsize=18)\n",
    "\n",
    "# # #             plt.show()\n",
    "\n",
    "\n",
    "# # #             traffic_jam3_5 = traffic_jam3.groupby('Year').error_frequency.mean()\n",
    "\n",
    "# # #             plt.bar(traffic_jam3_5.index, traffic_jam3_5)\n",
    "\n",
    "# # #             plt.title('Absence', fontsize=20)\n",
    "\n",
    "# # #             plt.xlabel('Year', fontsize=18)\n",
    "\n",
    "# # #             plt.ylabel('error_chance', fontsize=18)\n",
    "\n",
    "# # #             plt.show()\n",
    "            \n",
    "            \n",
    "# #             dice = X_train.index.tolist()\n",
    "# #             NAMS = traffic_jam.iloc[dice]\n",
    "# # #             NAMS['Species_18'].value_counts(normalize=True)\n",
    "# # #             NAMS['Year'].value_counts()\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "# #         mask = np.logical_not(np.equal(y_test, ws100_preds))\n",
    "# # #        print(f\"Elements wrong classified: {X_test[mask].index}\")\n",
    "# #         potcast.extend(mask.index.tolist())\n",
    "# # #         print(f\"Prediction by the model for each of those elements: {predictions[mask]}\")\n",
    "# # #         print(f\"Actual value for each of those elements: {np.asarray(y_test)[mask]}\")\n",
    "\n",
    "# #         allpot.extend(potcast)\n",
    "\n",
    "        \n",
    "\n",
    "    print()\n",
    "    print()\n",
    "    print()\n",
    "    print('<<<<주요종만sopr>>>>')\n",
    "\n",
    "    accuracys/len(pot)\n",
    "    precisions/len(pot)\n",
    "    recalls/len(pot)\n",
    "    f1s/len(pot)\n",
    "    roc_aucs/len(pot)\n",
    "               \n",
    "    T1 = T1 + (accuracys/len(pot))\n",
    "    T2 = T2 + (precisions/len(pot))\n",
    "    T3 = T3 + (recalls/len(pot))\n",
    "    T4 = T4 + (f1s/len(pot))\n",
    "    T5 = T5 + (roc_aucs/len(pot))\n",
    "    \n",
    "    accuracys = 0\n",
    "    precisions = 0\n",
    "    recalls = 0\n",
    "    f1s = 0\n",
    "    roc_aucs = 0\n",
    "           \n",
    "T1/len(hot)\n",
    "T2/len(hot)\n",
    "T3/len(hot)\n",
    "T4/len(hot)\n",
    "T5/len(hot)\n",
    "\n",
    "captain_Q_all = captain_Q_all.reset_index(drop=True)\n",
    "captain_jam_all = captain_Q_all[['name', 'shap_index']][0:len(LABEL)].copy()\n",
    "captain_Q_75 = captain_Q_75.reset_index(drop=True)\n",
    "captain_jam_75 = captain_Q_75[['name', 'shap_index']][0:len(LABEL)].copy()\n",
    "captain_Q_95 = captain_Q_95.reset_index(drop=True)\n",
    "captain_jam_95 = captain_Q_95[['name', 'shap_index']][0:len(LABEL)].copy()\n",
    "\n",
    "\n",
    "a=0\n",
    "for i in LABEL:\n",
    "    try:\n",
    "        captain_jam_all['name'][a] = i\n",
    "        knife = captain_Q_all[captain_Q_all['col_name'] == i]['feature_importance_vals'].mean()\n",
    "        captain_jam_all['shap_index'][a] = knife\n",
    "        a=a+1\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "a=0\n",
    "for i in LABEL:\n",
    "    try:\n",
    "        captain_jam_75['name'][a] = i\n",
    "        knife = captain_Q_75[captain_Q_75['col_name'] == i]['feature_importance_vals'].mean()\n",
    "        captain_jam_75['shap_index'][a] = knife\n",
    "        a=a+1\n",
    "    \n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "a=0\n",
    "for i in LABEL:\n",
    "    try:\n",
    "        captain_jam_95['name'][a] = i\n",
    "        knife = captain_Q_95[captain_Q_95['col_name'] == i]['feature_importance_vals'].mean()\n",
    "        captain_jam_95['shap_index'][a] = knife\n",
    "        a=a+1\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "\n",
    "captain_jam_all = captain_jam_all.sort_values(by='shap_index', ascending=False)\n",
    "captain_jam_all\n",
    "captain_jam_75 = captain_jam_75.sort_values(by='shap_index', ascending=False)\n",
    "captain_jam_75\n",
    "captain_jam_95 = captain_jam_95.sort_values(by='shap_index', ascending=False)\n",
    "captain_jam_95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6443e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "captain_jam_all.to_csv('./nov_3MCs_captain_jam_all.csv')\n",
    "# captain_jam_75.to_csv('./nov_3MCs_captain_jam_75.csv')\n",
    "# captain_jam_95.to_csv('./nov_3MCs_captain_jam_95.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5890a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433d01be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261d2f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 6)) # 축b 반환\n",
    "plot_importance(xgb_wrapper, ax=ax) # 학습이 된 xgb_model과 축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadc61d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "allpot.extend(potcast)\n",
    "\n",
    "b = Counter(potcast)\n",
    "\n",
    "taggerz = pd.DataFrame.from_dict(b, orient ='index')\n",
    "taggerz.rename(columns={0:'error_frequency'}, inplace=True)\n",
    "\n",
    "traffic_jam = pd.merge(c92zet, taggerz, left_index=True, right_index=True, how='left')\n",
    "traffic_jam = traffic_jam.fillna(0)\n",
    "\n",
    "traffic_jam['error_frequency']\n",
    "\n",
    "traffic_jam['error_chance'] = 0\n",
    "traffic_jam['error_chance'][:quiz] = traffic_jam['error_frequency'][:quiz]/(len(hot)*len(pot))\n",
    "traffic_jam['error_chance'][quiz:] = traffic_jam['error_frequency'][quiz:]/((len(c92_copy2[:quiz].index)*(len(hot)*len(pot)))/len(c92_copy2[quiz:].index))\n",
    "traffic_jam['error_chance']\n",
    "\n",
    "traffic_jam.sort_values(by='error_chance', axis=0, ascending=False, inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab82be3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic_jam2 = traffic_jam[traffic_jam.target==1].sort_values(by='Year', axis=0, ascending=False, inplace=False)\n",
    "\n",
    "traffic_jam3 = traffic_jam[traffic_jam.target==0].sort_values(by='Year', axis=0, ascending=False, inplace=False)\n",
    "\n",
    "\n",
    "traffic_jam2_5 = traffic_jam2.groupby('Year').error_frequency.mean()\n",
    "\n",
    "plt.bar(traffic_jam2_5.index, traffic_jam2_5)\n",
    "\n",
    "plt.title('Presence', fontsize=20)\n",
    "\n",
    "plt.xlabel('Year', fontsize=18)\n",
    "\n",
    "plt.ylabel('error_chance', fontsize=18)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "traffic_jam3_5 = traffic_jam3.groupby('Year').error_frequency.mean()\n",
    "\n",
    "plt.bar(traffic_jam3_5.index, traffic_jam3_5)\n",
    "\n",
    "plt.title('Absence', fontsize=20)\n",
    "\n",
    "plt.xlabel('Year', fontsize=18)\n",
    "\n",
    "plt.ylabel('error_chance', fontsize=18)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b492d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs()\n",
    "explainer = shap.TreeExplainer(xgb_wrapper)\n",
    "shap_values = explainer.shap_values(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e1c472",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.force_plot(explainer.expected_value, shap_values[4, :], X_train.iloc[4, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd25719d",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs()\n",
    "shap.force_plot(explainer.expected_value, shap_values, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8e0343",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs()\n",
    "# 총 13개 특성의 Shapley value를 절댓값 변환 후 각 특성마다 더함 -> np.argsort()는 작은 순서대로 정렬, 큰 순서대로 정렬하려면\n",
    "# 앞에 마이너스(-) 기호를 붙임\n",
    "top_inds = np.argsort(-np.sum(np.abs(shap_values), 0))\n",
    "\n",
    "# 영향력 top 2 컬럼\n",
    "for i in range(2):\n",
    "    shap.dependence_plot(top_inds[i], shap_values, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb551a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, X_train)\n",
    "\n",
    "\n",
    "# 진단1 : convergens와 septempunctata가 정확도를 낮추는 원인일 것(why? 크고 작고의 영향이 서로 구분되지 않음).\n",
    "# -> 이들의 영향을 구분하도록 데이터 프레임을 짜는 것이 관건\n",
    "## --> absence data 때문?\n",
    "## --> 다른 변수 추가로 해결 가능?\n",
    "\n",
    "\n",
    "# 진단2 : 잘 쓰이지 않는 변수\n",
    "# -> Subcoccinella_vigintiquatuorpunctata\n",
    "# -> Hippodamia_quinquesignata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1274f423",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, X_train, plot_type='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23ab47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_interaction_values = explainer.shap_interaction_values(X_train)\n",
    "shap.summary_plot(shap_interaction_values, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805669b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.dependence_plot(\n",
    "    ('Hippodamia_convergens_18', 'Coccinella_septempunctata_18'),\n",
    "    shap_interaction_values, X_train,\n",
    "    display_features=X_train\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c3d168",
   "metadata": {},
   "source": [
    "# 그 밖의 통계 결과"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e62857",
   "metadata": {},
   "source": [
    "## 결정트리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce28275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결정트리\n",
    "\n",
    "## 4년\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(MCz, c92sr2_y, test_size=0.2, random_state=3)\n",
    "\n",
    "dt_clf = DecisionTreeClassifier(random_state = 4)\n",
    "\n",
    "dt_clf.fit(X_train, y_train) # label 과 feature가 함께 들어감\n",
    "\n",
    "pred = dt_clf.predict(X_test) # label 없이 feature만 들어감\n",
    "\n",
    "print('s 예측 정확도: {0:4f}'.format(accuracy_score(y_test,pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5714357f",
   "metadata": {},
   "outputs": [],
   "source": [
    "viz = dtreeviz(xgb_wrapper[0], \n",
    "               x_data=X_train,\n",
    "               y_data=y_train,\n",
    "               target_name='class',\n",
    "               feature_names=MCz.columns, \n",
    "               class_names=[0,1], \n",
    "               title=\"Decision Tree - Iris data set\")\n",
    "viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4678a573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot decision tree\n",
    "from dtreeviz.trees import dtreeviz # remember to load the package\n",
    "\n",
    "viz = dtreeviz(dt_clf, \n",
    "               x_data=X_train,\n",
    "               y_data=y_train,\n",
    "               target_name='class',\n",
    "               feature_names=MCz.columns, \n",
    "               class_names=[0,1], \n",
    "               title=\"Decision Tree - Iris data set\")\n",
    "viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852099de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random state를 500번 셔플한 결과들의 평균\n",
    "pot = list(range(0, 501))\n",
    "\n",
    "accuracys = 0\n",
    "precisions = 0\n",
    "recalls = 0\n",
    "f1s = 0\n",
    "\n",
    "\n",
    "for i in pot:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(MCz, c92sr2_y, test_size=0.2, random_state=i)\n",
    "\n",
    "    dt_clf = DecisionTreeClassifier(random_state = i)\n",
    "\n",
    "    dt_clf.fit(X_train, y_train) # label 과 feature가 함께 들어감\n",
    "\n",
    "    pred = dt_clf.predict(X_test) # label 없이 feature만 들어감\n",
    "\n",
    "    # 예측 성능 평가\n",
    "    a, b, c, d = get_clf_eval_num_easy(y_test, pred)\n",
    "    accuracys = accuracys + a\n",
    "    precisions = precisions + b\n",
    "    recalls = recalls + c\n",
    "    f1s = f1s + d\n",
    "    \n",
    "print()\n",
    "print()\n",
    "print()\n",
    "print('<<<<주요종만sopr>>>>')\n",
    "\n",
    "accuracys/len(pot)\n",
    "precisions/len(pot)\n",
    "recalls/len(pot)\n",
    "f1s/len(pot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0ad82e",
   "metadata": {},
   "source": [
    "## 앙상블학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4719d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 로지스틱 회귀 / KNN(K-최근접 이웃 알고리즘)\n",
    "lr_clf = LogisticRegression(solver='liblinear') # 옵션\n",
    "knn_clf = KNeighborsClassifier(n_neighbors=8) # 옵션\n",
    "\n",
    "# 둘이 합쳐서\n",
    "vo_clf = VotingClassifier(estimators=[('LR', lr_clf), ('KNN', knn_clf)], voting='soft') # 옵션\n",
    "\n",
    "\n",
    "# 3. 학습 (데이터세트 분리) \n",
    "\n",
    "print()\n",
    "print()\n",
    "print()\n",
    "print('<<<<s>>>>>')\n",
    "\n",
    "# 데이터 세트 분리\n",
    "X_train, X_test, y_train, y_test = train_test_split(MCz, c92sr2_y, test_size=0.2 , random_state= 156) # 정확도는 0.1일 때 가장 높음\n",
    "\n",
    "#### voting 분류기\n",
    "# 학습/예측/평가\n",
    "vo_clf.fit(X_train,y_train)\n",
    "pred = vo_clf.predict(X_test)\n",
    "# 평가\n",
    "print('Voting 분류기 정확도: {0:.4f}'.format(accuracy_score(y_test , pred)))\n",
    "\n",
    "\n",
    "#### 개별일 때의 결과\n",
    "# 로지스틱 회귀와 KNN 각 개별 모델로 학습/예측/평가\n",
    "classifiers = [lr_clf,knn_clf]\n",
    "\n",
    "for classifier in classifiers:\n",
    "    classifier.fit(X_train , y_train)\n",
    "    pred = classifier.predict(X_test)\n",
    "    class_name= classifier.__class__.__name__  # __class__ : 클래스명  # __name__ : 클래스명 추출\n",
    "    print('{0} 정확도: {1:.4f}'.format(class_name, accuracy_score(y_test , pred)))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cda0c99",
   "metadata": {},
   "source": [
    "## 랜덤 포레스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd3b224",
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "print()\n",
    "print()\n",
    "print('<<<<주요종만sopr>>>>')\n",
    "\n",
    "# random state를 500번 셔플한 결과들의 평균\n",
    "\n",
    "hot = list(range (0, 100))\n",
    "pot = list(range(0, 100))\n",
    "           \n",
    "T1 = 0\n",
    "T2 = 0\n",
    "T3 = 0\n",
    "T4 = 0\n",
    "T5 = 0\n",
    "\n",
    "accuracys = 0\n",
    "precisions = 0\n",
    "recalls = 0\n",
    "f1s = 0\n",
    "roc_aucs = 0\n",
    "           \n",
    "for k in tqdm(hot):\n",
    "    sample = c92_copy2[:2005].sample(n=294, random_state = k)\n",
    "    c92_copy22 = pd.concat([c92_copy2[2006:], sample])\n",
    "    \n",
    "    c92sr2_y = c92_copy22.target\n",
    "    MCz= c92_copy22[DJ]\n",
    "    # MCz.drop(columns = 'target', inplace=True)\n",
    "\n",
    "    for i in pot:\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(MCz, c92sr2_y, test_size=0.1, random_state= i) # 정확도는 0.1일 때 가장 높음\n",
    "\n",
    "        rf_clf = RandomForestClassifier(random_state=i)\n",
    "        rf_clf.fit(X_train, y_train)\n",
    "        pred = rf_clf.predict(X_test)\n",
    "\n",
    "        # 예측 성능 평가\n",
    "        a, b, c, d = get_clf_eval_num_easy(y_test, pred)\n",
    "        accuracys = accuracys + a\n",
    "        precisions = precisions + b\n",
    "        recalls = recalls + c\n",
    "        f1s = f1s + d\n",
    "        roc_aucs = roc_aucs + e\n",
    "\n",
    "    print()\n",
    "    print()\n",
    "    print()\n",
    "    print('<<<<주요종만sopr>>>>')\n",
    "\n",
    "    accuracys/len(pot)\n",
    "    precisions/len(pot)\n",
    "    recalls/len(pot)\n",
    "    f1s/len(pot)\n",
    "    roc_aucs/len(pot)\n",
    "               \n",
    "    T1 = T1 + (accuracys/len(pot))\n",
    "    T2 = T2 + (precisions/len(pot))\n",
    "    T3 = T3 + (recalls/len(pot))\n",
    "    T4 = T4 + (f1s/len(pot))\n",
    "    T5 = T5 + (roc_aucs/len(pot))\n",
    "    \n",
    "    accuracys = 0\n",
    "    precisions = 0\n",
    "    recalls = 0\n",
    "    f1s = 0\n",
    "    roc_aucs = 0\n",
    "           \n",
    "T1/len(hot)\n",
    "T2/len(hot)\n",
    "T3/len(hot)\n",
    "T4/len(hot)\n",
    "T5/len(hot)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3cff21",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf1 = RandomForestClassifier(n_estimators=1000, max_depth=4, min_samples_leaf=8, \\\n",
    "                                 min_samples_split=8, random_state=0)\n",
    "rf_clf1.fit(X_train , y_train)\n",
    "pred = rf_clf1.predict(X_test)\n",
    "print('예측 정확도: {0:.4f}'.format(accuracy_score(y_test , pred)))\n",
    "\n",
    "\n",
    "# 앞으로 계속 중요도 시각화를 죽 계속할 건데 이 코드를 계속 비슷하게 사용한다고 생각하면 됨\n",
    "ftr_importances_values = rf_clf1.feature_importances_\n",
    "ftr_importances = pd.Series(ftr_importances_values, index=X_train.columns)\n",
    "# sort_values() 쉽게 하기 위해서 시리즈로 만들고, \n",
    "# 최고 중요도가 높은 20개 피처들만 추출\n",
    "ftr_top20 = ftr_importances.sort_values(ascending=False)[:20]\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.title('Feature importances Top 20')\n",
    "# x축은 중요도 값, y축은 ftr_top20 시리즈의 index\n",
    "sns.barplot(x=ftr_top20 , y = ftr_top20.index)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbea4b2",
   "metadata": {},
   "source": [
    "### GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2a1869",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# GBM 수행 시간을 측정하기 위한 time()객체 생성\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "\n",
    "#### 4년 \n",
    "\n",
    "print()\n",
    "print()\n",
    "print()\n",
    "print('<<<<s>>>>>')\n",
    "\n",
    "# 데이터 세트 분리\n",
    "X_train, X_test, y_train, y_test = train_test_split(MCz, c92sr2_y, test_size=0.2 , random_state= 156) # 정확도는 0.1일 때 가장 높음\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "gb_clf = GradientBoostingClassifier(random_state=99) # 모델 객체 생성\n",
    "gb_clf.fit(X_train, y_train) # 학습\n",
    "pred = gb_clf.predict(X_test)\n",
    "\n",
    "gb_accuracy = accuracy_score(y_test, pred)\n",
    "\n",
    "print('GBM 정확도: {0:.4f}'.format(gb_accuracy))\n",
    "print(\"GBM 수행 시간: {0:.1f} 초 \".format(time.time() - start_time))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9400174a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "print()\n",
    "print()\n",
    "print('<<<<주요종만sopr>>>>')\n",
    "\n",
    "# random state를 500번 셔플한 결과들의 평균\n",
    "\n",
    "hot = list(range (0, 100))\n",
    "pot = list(range(0, 100))\n",
    "           \n",
    "T1 = 0\n",
    "T2 = 0\n",
    "T3 = 0\n",
    "T4 = 0\n",
    "T5 = 0\n",
    "\n",
    "accuracys = 0\n",
    "precisions = 0\n",
    "recalls = 0\n",
    "f1s = 0\n",
    "roc_aucs = 0\n",
    "           \n",
    "for k in tqdm(hot):\n",
    "    sample = c92_copy2[:2005].sample(n=294, random_state = k)\n",
    "    c92_copy22 = pd.concat([c92_copy2[2006:], sample])\n",
    "    \n",
    "    c92sr2_y = c92_copy22.target\n",
    "    MCz= c92_copy22[DJ]\n",
    "    # MCz.drop(columns = 'target', inplace=True)\n",
    "\n",
    "    for i in pot:\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(MCz, c92sr2_y, test_size=0.1, random_state= i) # 정확도는 0.1일 때 가장 높음\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        gb_clf = GradientBoostingClassifier(random_state=i) # 모델 객체 생성\n",
    "        gb_clf.fit(X_train, y_train) # 학습\n",
    "        pred = gb_clf.predict(X_test)\n",
    "\n",
    "        # 예측 성능 평가\n",
    "        a, b, c, d = get_clf_eval_num_easy(y_test, pred)\n",
    "        accuracys = accuracys + a\n",
    "        precisions = precisions + b\n",
    "        recalls = recalls + c\n",
    "        f1s = f1s + d\n",
    "        roc_aucs = roc_aucs + e\n",
    "\n",
    "    print()\n",
    "    print()\n",
    "    print()\n",
    "    print('<<<<주요종만sopr>>>>')\n",
    "\n",
    "    accuracys/len(pot)\n",
    "    precisions/len(pot)\n",
    "    recalls/len(pot)\n",
    "    f1s/len(pot)\n",
    "    roc_aucs/len(pot)\n",
    "               \n",
    "    T1 = T1 + (accuracys/len(pot))\n",
    "    T2 = T2 + (precisions/len(pot))\n",
    "    T3 = T3 + (recalls/len(pot))\n",
    "    T4 = T4 + (f1s/len(pot))\n",
    "    T5 = T5 + (roc_aucs/len(pot))\n",
    "    \n",
    "    accuracys = 0\n",
    "    precisions = 0\n",
    "    recalls = 0\n",
    "    f1s = 0\n",
    "    roc_aucs = 0\n",
    "           \n",
    "T1/len(hot)\n",
    "T2/len(hot)\n",
    "T3/len(hot)\n",
    "T4/len(hot)\n",
    "T5/len(hot)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28448a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lightgbm을 구현하여 shap value를 예측할 것\n",
    "# ligthgbm 구현\n",
    "\n",
    "# library\n",
    "import lightgbm as lgb  # 없을 경우 cmd/anaconda prompt에서 install\n",
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# lightgbm model\n",
    "lgb_dtrain = lgb.Dataset(data = train_x, label = train_y) # LightGBM 모델에 맞게 변환\n",
    "lgb_param = {'max_depth': 10,\n",
    "            'learning_rate': 0.01, # Step Size\n",
    "            'n_estimators': 1000, # Number of trees\n",
    "            'objective': 'regression'} # 목적 함수 (L2 Loss)\n",
    "lgb_model = lgb.train(params = lgb_param, train_set = lgb_dtrain) # 학습 진행\n",
    "lgb_model_predict = lgb_model.predict(test_x) # test data 예측\n",
    "print(\"RMSE: {}\".format(sqrt(mean_squared_error(lgb_model_predict, test_y)))) # RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092ce1b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0639bf7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd75e449",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3650e20c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sklearn01",
   "language": "python",
   "name": "sklearn01"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "459px",
    "left": "1275px",
    "top": "246.125px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
